"""
ç§Ÿå±‹è¡Œæƒ…åˆ†æç³»çµ± - ç‰ˆæœ¬æ§åˆ¶ API v5.1
æ”¯æŒé€±æ¬¡ç®¡ç†ã€å‹•ç•«æ’­æ”¾ã€ç•™ç½®æ™‚é–“è‘—è‰²ã€å»ºç¯‰é¡å‹ç¯©é¸å’Œé€²éšæ¨¡å¼
æ”¯æ´æ¡ˆä»¶ç·¨è™Ÿï¼ˆproperty_idï¼‰é€²è¡Œç²¾ç¢ºæˆ¿æºè¿½è¹¤
æ”¯æ´åŸå§‹ CSV æ ¼å¼ï¼ˆåº¦åˆ†ç§’åº§æ¨™è‡ªå‹•è½‰æ›ï¼‰
"""

import sqlite3
import json
import os
import re
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional
import math

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="ç§Ÿå±‹è¡Œæƒ…åˆ†æ API v5.1")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============ æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶ ============

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚åˆå§‹åŒ–æ•¸æ“šåº«ä¸¦è‡ªå‹•å°å…¥ CSV"""
    init_database()
    auto_import_csv_files()


# æ•¸æ“šåº«è·¯å¾‘
DB_PATH = os.path.join(os.path.dirname(__file__), "rental.db")

# ============ æ•¸æ“šåº«åˆå§‹åŒ– ============

def init_database():
    """åˆå§‹åŒ–æ•¸æ“šåº«ï¼Œæ·»åŠ ç‰ˆæœ¬æ§åˆ¶å­—æ®µå’Œå»ºç¯‰é¡å‹"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS versions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            week_id TEXT UNIQUE NOT NULL,
            upload_date TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS properties (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            property_id TEXT UNIQUE,
            title TEXT,
            address TEXT,
            rent_monthly INTEGER,
            area REAL,
            room_type TEXT,
            floor TEXT,
            latitude REAL,
            longitude REAL,
            renovation_status TEXT,
            first_published_date TEXT,
            upload_week TEXT,
            status TEXT DEFAULT 'active',
            building_type TEXT DEFAULT 'apartment',
            deleted_date TEXT
        )
    """)
    
    cursor.execute("PRAGMA table_info(properties)")
    columns = {row[1] for row in cursor.fetchall()}
    
    if 'property_id' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN property_id TEXT")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_property_id ON properties(property_id)")
    
    if 'first_published_date' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN first_published_date TEXT")
    
    if 'deleted_date' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN deleted_date TEXT")
    
    if 'upload_week' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN upload_week TEXT")
    
    if 'status' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN status TEXT DEFAULT 'active'")
    
    if 'building_type' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN building_type TEXT DEFAULT 'apartment'")
    
    conn.commit()
    conn.close()

# ============ å·¥å…·å‡½æ•¸ ============

def get_week_id(date: datetime = None) -> str:
    if date is None:
        date = datetime.now()
    year = date.year % 100
    week = date.isocalendar()[1]
    return f"{year:02d}{week:02d}"

def extract_building_type_from_filename(filename: str) -> str:
    if 'é›»æ¢¯å¤§æ¨“' in filename or 'é›»æ¢¯' in filename:
        return 'building'
    elif 'å…¬å¯“' in filename:
        return 'apartment'
    return 'unknown'

def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    R = 6371000
    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    delta_lat = math.radians(lat2 - lat1)
    delta_lon = math.radians(lon2 - lon1)
    a = math.sin(delta_lat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R * c

def calculate_weeks_since_published(first_published_date: str) -> int:
    if not first_published_date:
        return 0
    try:
        first_date = datetime.strptime(first_published_date, "%Y-%m-%d")
        now = datetime.now()
        delta = now - first_date
        weeks = delta.days // 7
        return max(0, weeks)
    except:
        return 0

def parse_dms_coordinate(coord_str: str):
    """
    è§£æåº¦åˆ†ç§’æ ¼å¼çš„åº§æ¨™å­—ä¸²
    æ”¯æ´æ ¼å¼: 25Â°0'17"N 121Â°29'47"E æˆ– 25Â°0'17"N, 121Â°29'47"E
    è¿”å›: (ç·¯åº¦, ç¶“åº¦) æˆ– (0, 0) å¦‚æœè§£æå¤±æ•—
    """
    if not coord_str or coord_str == 'nan':
        return 0, 0
    
    try:
        # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œé€—è™Ÿ
        coord_str = str(coord_str).strip()
        
        # åŒ¹é…åº¦åˆ†ç§’æ ¼å¼: 25Â°0'17"N æˆ– 121Â°29'47"E
        pattern = r"(\d+)Â°(\d+)'(\d+(?:\.\d+)?)\"([NSEW])"
        matches = re.findall(pattern, coord_str)
        
        if len(matches) >= 2:
            lat_match = None
            lng_match = None
            
            for match in matches:
                deg, min_, sec, direction = match
                if direction in ['N', 'S']:
                    lat_match = (float(deg), float(min_), float(sec), direction)
                elif direction in ['E', 'W']:
                    lng_match = (float(deg), float(min_), float(sec), direction)
            
            if lat_match and lng_match:
                # è¨ˆç®—åé€²ä½ç·¯åº¦
                lat = lat_match[0] + lat_match[1]/60 + lat_match[2]/3600
                if lat_match[3] == 'S':
                    lat = -lat
                
                # è¨ˆç®—åé€²ä½ç¶“åº¦
                lng = lng_match[0] + lng_match[1]/60 + lng_match[2]/3600
                if lng_match[3] == 'W':
                    lng = -lng
                
                return round(lat, 6), round(lng, 6)
        
        return 0, 0
    except Exception as e:
        print(f"åº§æ¨™è§£æéŒ¯èª¤: {coord_str} - {e}")
        return 0, 0

def extract_district_from_address(address: str) -> tuple:
    """
    å¾åœ°å€ä¸­æå–åŸå¸‚å’Œå€åŸŸ
    è¿”å›: (åŸå¸‚, å€åŸŸ, æ¸…ç†å¾Œçš„åœ°å€)
    """
    if not address:
        return '', '', ''
    
    address = str(address).strip()
    
    # å®šç¾©åŸå¸‚å’Œå€åŸŸçš„æ˜ å°„
    cities = ['è‡ºåŒ—å¸‚', 'å°åŒ—å¸‚', 'æ–°åŒ—å¸‚', 'åŸºéš†å¸‚', 'æ¡ƒåœ’å¸‚', 'æ–°ç«¹å¸‚', 'æ–°ç«¹ç¸£']
    
    # æ–°åŒ—å¸‚çš„å€åŸŸ
    new_taipei_districts = [
        'æ¿æ©‹å€', 'ä¸‰é‡å€', 'ä¸­å’Œå€', 'æ°¸å’Œå€', 'æ–°èŠå€', 'æ–°åº—å€', 'åœŸåŸå€',
        'è˜†æ´²å€', 'æ¨¹æ—å€', 'æ±æ­¢å€', 'é¶¯æ­Œå€', 'ä¸‰å³½å€', 'æ·¡æ°´å€', 'ç‘èŠ³å€',
        'äº”è‚¡å€', 'æ³°å±±å€', 'æ—å£å€', 'æ·±å‘å€', 'çŸ³ç¢‡å€', 'åªæ—å€', 'ä¸‰èŠå€',
        'çŸ³é–€å€', 'å…«é‡Œå€', 'å¹³æºªå€', 'é›™æºªå€', 'è²¢å¯®å€', 'é‡‘å±±å€', 'è¬é‡Œå€',
        'çƒä¾†å€'
    ]
    
    city = ''
    district = ''
    
    # æª¢æŸ¥åœ°å€æ˜¯å¦ä»¥åŸå¸‚é–‹é ­
    for c in cities:
        if address.startswith(c):
            city = c
            address = address[len(c):]
            break
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«å€åŸŸ
    for d in new_taipei_districts:
        if address.startswith(d):
            district = d
            if not city:
                city = 'æ–°åŒ—å¸‚'
            break
        elif d in address:
            district = d
            if not city:
                city = 'æ–°åŒ—å¸‚'
            break
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°åŸå¸‚ä½†æ‰¾åˆ°äº†å€åŸŸï¼Œæ ¹æ“šå€åŸŸæ¨æ–·åŸå¸‚
    if not city and district:
        city = 'æ–°åŒ—å¸‚'
    
    # çµ„åˆå®Œæ•´åœ°å€
    full_address = city + address if city and not address.startswith(city) else address
    if city and district and not full_address.startswith(city):
        full_address = city + full_address
    
    return city, district, full_address

# ============ CSV å°å…¥åŠŸèƒ½ ============

def auto_import_csv_files():
    import pandas as pd
    
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "upload"),
        "/app/upload",
        "./upload",
        os.path.join(os.getcwd(), "upload")
    ]
    
    upload_dir = None
    for path in possible_paths:
        if os.path.exists(path):
            upload_dir = path
            print(f"âœ… æ‰¾åˆ° upload è³‡æ–™å¤¾: {upload_dir}")
            break
    
    if upload_dir is None:
        upload_dir = possible_paths[0]
        if not os.path.exists(upload_dir):
            os.makedirs(upload_dir)
            print(f"âœ… å·²å‰µå»º upload è³‡æ–™å¤¾: {upload_dir}")
            return
    
    csv_files = [f for f in os.listdir(upload_dir) if f.endswith('.csv')]
    
    if not csv_files:
        print("âš ï¸  upload è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆï¼Œé–‹å§‹å°å…¥...")
    
    week_data = {}
    
    for csv_filename in csv_files:
        try:
            csv_path = os.path.join(upload_dir, csv_filename)
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            
            # å¾æ–‡ä»¶åæå–å»ºç¯‰é¡å‹
            filename_building_type = extract_building_type_from_filename(csv_filename)
            
            # å¾æ–‡ä»¶åæå–é€±æ¬¡
            filename_week = None
            week_match = re.search(r'_(\d{4})\.csv$', csv_filename)
            if week_match:
                filename_week = week_match.group(1)
            # ä¹Ÿå˜—è©¦åŒ¹é… _2603_merged.csv æ ¼å¼
            week_match2 = re.search(r'_(\d{4})_merged\.csv$', csv_filename)
            if week_match2:
                filename_week = week_match2.group(1)
            
            print(f"  è™•ç†: {csv_filename} (å»ºç¯‰é¡å‹: {filename_building_type}, é€±æ¬¡: {filename_week})")
            
            for _, row in df.iterrows():
                # æå–æ¡ˆä»¶ç·¨è™Ÿ
                property_id = row.get('æ¡ˆä»¶ç·¨è™Ÿ', '')
                if pd.isna(property_id) or not property_id:
                    continue
                property_id = str(int(property_id) if isinstance(property_id, float) else property_id)
                
                # æå–æ¨™é¡Œ
                title = str(row.get('æ¨™é¡Œ', ''))
                
                # æå–åœ°å€ä¸¦è£œå……åŸå¸‚å€åŸŸ
                raw_address = str(row.get('åœ°å€', ''))
                city, district, address = extract_district_from_address(raw_address)
                
                # ç§Ÿé‡‘
                rent = row.get('ç§Ÿé‡‘', 0)
                if pd.isna(rent):
                    rent = 0
                rent = int(rent)
                
                # åªæ•¸ï¼ˆæ”¯æ´ã€Œåªæ•¸ã€å’Œã€Œå¡æ•¸ã€å…©ç¨®æ¬„ä½åï¼‰
                area = row.get('åªæ•¸', row.get('å¡æ•¸', 0))
                if pd.isna(area):
                    area = 0
                area = float(area)
                
                # æˆ¿å‹
                room_type = str(row.get('æˆ¿å‹', ''))
                if room_type == 'nan':
                    room_type = ''
                
                # æ¨“å±¤
                floor = str(row.get('æ¨“å±¤', ''))
                if floor == 'nan':
                    floor = ''
                
                # å»ºç¯‰é¡å‹ï¼ˆå„ªå…ˆå¾ CSV æ¬„ä½è®€å–ï¼Œå¦å‰‡å¾æ–‡ä»¶åæ¨æ–·ï¼‰
                csv_building_type = str(row.get('å»ºç¯‰é¡å‹', ''))
                if csv_building_type and csv_building_type != 'nan':
                    if 'é›»æ¢¯' in csv_building_type or 'å¤§æ¨“' in csv_building_type:
                        building_type = 'building'
                    elif 'å…¬å¯“' in csv_building_type:
                        building_type = 'apartment'
                    else:
                        building_type = filename_building_type
                else:
                    building_type = filename_building_type
                
                # åº§æ¨™è™•ç†ï¼šæ”¯æ´å…©ç¨®æ ¼å¼
                # 1. æ–°æ ¼å¼ï¼šç¨ç«‹çš„ã€Œç·¯åº¦ã€å’Œã€Œç¶“åº¦ã€æ¬„ä½
                # 2. åŸå§‹æ ¼å¼ï¼šã€Œåº§æ¨™ã€æ¬„ä½ï¼ˆåº¦åˆ†ç§’æ ¼å¼ï¼‰
                latitude = 0
                longitude = 0
                
                if 'ç·¯åº¦' in df.columns and 'ç¶“åº¦' in df.columns:
                    # æ–°æ ¼å¼
                    lat_val = row.get('ç·¯åº¦', 0)
                    lng_val = row.get('ç¶“åº¦', 0)
                    if not pd.isna(lat_val) and not pd.isna(lng_val):
                        latitude = float(lat_val)
                        longitude = float(lng_val)
                
                if latitude == 0 and longitude == 0 and 'åº§æ¨™' in df.columns:
                    # åŸå§‹æ ¼å¼ï¼šåº¦åˆ†ç§’
                    coord_str = row.get('åº§æ¨™', '')
                    if not pd.isna(coord_str):
                        latitude, longitude = parse_dms_coordinate(str(coord_str))
                
                # é€±æ¬¡ï¼ˆæ”¯æ´ã€Œé€±æ¬¡ã€å’Œã€Œå¹´é€±ã€å…©ç¨®æ¬„ä½åï¼‰
                week_id = row.get('é€±æ¬¡', row.get('å¹´é€±', ''))
                if pd.isna(week_id) or not week_id:
                    week_id = filename_week if filename_week else get_week_id()
                week_id = str(week_id)
                if week_id.endswith('.0'):
                    week_id = week_id[:-2]
                # ç¢ºä¿é€±æ¬¡æ˜¯4ä½æ•¸å­—
                if len(week_id) == 4 and week_id.isdigit():
                    pass  # æ ¼å¼æ­£ç¢º
                else:
                    week_id = filename_week if filename_week else get_week_id()
                
                # è£ä¿®ç‹€æ…‹
                renovation_status = str(row.get('è£ä¿®ç‹€æ…‹', 'unknown'))
                if renovation_status == 'nan':
                    renovation_status = 'unknown'
                
                # è·³éç„¡æ•ˆæ•¸æ“š
                if not address or rent <= 0:
                    continue
                
                # æŒ‰é€±æ¬¡åˆ†çµ„
                if week_id not in week_data:
                    week_data[week_id] = []
                
                week_data[week_id].append({
                    'property_id': property_id,
                    'title': title,
                    'address': address,
                    'rent': rent,
                    'area': area,
                    'room_type': room_type,
                    'floor': floor,
                    'latitude': latitude,
                    'longitude': longitude,
                    'building_type': building_type,
                    'renovation_status': renovation_status
                })
            
            print(f"  âœ“ è®€å–å®Œæˆ: {csv_filename} ({len(df)} è¡Œ)")
        
        except Exception as e:
            print(f"  âš ï¸  {csv_filename} è®€å–å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not week_data:
        print("âŒ æ²’æœ‰æˆåŠŸè®€å–ä»»ä½•æ•¸æ“š")
        return
    
    # å°å…¥åˆ°æ•¸æ“šåº«
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    upload_date = datetime.now().strftime("%Y-%m-%d")
    total_new = 0
    total_updated = 0
    
    for week_id, properties in sorted(week_data.items()):
        print(f"\nğŸ“… è™•ç†é€±æ¬¡ {week_id}...")
        
        cursor.execute("INSERT OR REPLACE INTO versions (week_id, upload_date) VALUES (?, ?)", (week_id, upload_date))
        
        current_week_ids = set()
        
        # å»é‡ï¼ˆåŸºæ–¼ property_idï¼‰
        seen_ids = set()
        unique_properties = []
        for prop in properties:
            if prop['property_id'] not in seen_ids:
                seen_ids.add(prop['property_id'])
                unique_properties.append(prop)
        
        print(f"  å»é‡å¾Œ: {len(unique_properties)} ç­†")
        
        week_new = 0
        week_updated = 0
        
        for prop in unique_properties:
            try:
                current_week_ids.add(prop['property_id'])
                
                cursor.execute("SELECT id, first_published_date FROM properties WHERE property_id = ?", (prop['property_id'],))
                result = cursor.fetchone()
                
                if result:
                    cursor.execute("""
                        UPDATE properties 
                        SET title = ?, address = ?, rent_monthly = ?, area = ?,
                            room_type = ?, floor = ?, latitude = ?, longitude = ?,
                            building_type = ?, renovation_status = ?,
                            status = 'active', upload_week = ?, deleted_date = NULL
                        WHERE property_id = ?
                    """, (prop['title'], prop['address'], prop['rent'], prop['area'],
                          prop['room_type'], prop['floor'], prop['latitude'], prop['longitude'],
                          prop['building_type'], prop['renovation_status'],
                          week_id, prop['property_id']))
                    week_updated += 1
                else:
                    first_published_date = upload_date
                    cursor.execute("""
                        INSERT INTO properties 
                        (property_id, title, address, rent_monthly, area, room_type, floor, 
                         latitude, longitude, building_type, renovation_status, 
                         first_published_date, upload_week, status)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'active')
                    """, (prop['property_id'], prop['title'], prop['address'], prop['rent'], 
                          prop['area'], prop['room_type'], prop['floor'], 
                          prop['latitude'], prop['longitude'], prop['building_type'], 
                          prop['renovation_status'], first_published_date, week_id))
                    week_new += 1
            except Exception as e:
                print(f"    âš ï¸ å°å…¥å¤±æ•— {prop['property_id']}: {e}")
                continue
        
        print(f"  æ–°å¢: {week_new} ç­†, æ›´æ–°: {week_updated} ç­†")
        total_new += week_new
        total_updated += week_updated
        
        # æ¨™è¨˜æœ¬é€±æœªå‡ºç¾çš„æˆ¿æºç‚º deletedï¼ˆåªå°æœ€æ–°é€±æ¬¡åŸ·è¡Œï¼‰
        all_weeks = sorted(week_data.keys())
        if week_id == all_weeks[-1] and current_week_ids:
            placeholders = ','.join(['?' for _ in current_week_ids])
            cursor.execute(f"""
                UPDATE properties 
                SET status = 'deleted', deleted_date = ?
                WHERE status = 'active' 
                AND property_id NOT IN ({placeholders})
                AND upload_week < ?
            """, [upload_date] + list(current_week_ids) + [week_id])
            
            deleted_count = cursor.rowcount
            if deleted_count > 0:
                print(f"  æ¨™è¨˜ä¸‹æ¶: {deleted_count} ç­†")
    
    conn.commit()
    
    # çµ±è¨ˆæœ‰åº§æ¨™çš„æˆ¿æº
    cursor.execute("SELECT COUNT(*) FROM properties WHERE latitude != 0 AND longitude != 0")
    with_coords = cursor.fetchone()[0]
    cursor.execute("SELECT COUNT(*) FROM properties")
    total = cursor.fetchone()[0]
    
    conn.close()
    
    print(f"\nâœ… CSV å°å…¥å®Œæˆï¼")
    print(f"  ç¸½æ–°å¢: {total_new} ç­†")
    print(f"  ç¸½æ›´æ–°: {total_updated} ç­†")
    print(f"  æœ‰åº§æ¨™: {with_coords}/{total} ç­†")
    print(f"  é€±æ¬¡ç‰ˆæœ¬: {', '.join(sorted(week_data.keys()))}")

# ============ API ç«¯é» ============

@app.get("/api/versions")
async def get_versions():
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        conn.close()
        return {"status": "success", "versions": versions, "count": len(versions)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis_v4")
async def analysis_v4(
    address: str,
    distance_min: int = 300,
    distance_max: int = 3000,
    building_type: Optional[str] = None,
    room_type: Optional[str] = None,
    week_id: Optional[str] = None,
    lat: Optional[float] = None,
    lng: Optional[float] = None
):
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        if lat is not None and lng is not None and lat != 0 and lng != 0:
            query_lat, query_lon = lat, lng
        else:
            query_lat, query_lon = 25.0288, 121.4625
        
        query = "SELECT * FROM properties WHERE status IN ('active', 'deleted')"
        params = []
        
        if week_id:
            query += " AND upload_week <= ?"
            params.append(week_id)
        
        cursor.execute(query, params)
        all_properties = cursor.fetchall()
        
        cursor.execute("PRAGMA table_info(properties)")
        columns = {row[1]: row[0] for row in cursor.fetchall()}
        
        filtered_properties = []
        for prop in all_properties:
            lat_idx = columns.get('latitude')
            lng_idx = columns.get('longitude')
            if lat_idx is None or lng_idx is None:
                continue
            if prop[lat_idx] is None or prop[lng_idx] is None:
                continue
            if prop[lat_idx] == 0.0 and prop[lng_idx] == 0.0:
                continue
            
            prop_dict = {
                'id': prop[columns['id']],
                'property_id': prop[columns.get('property_id', columns['id'])] if 'property_id' in columns else None,
                'title': prop[columns['title']],
                'address': prop[columns['address']],
                'rent_monthly': prop[columns['rent_monthly']],
                'area': prop[columns['area']],
                'floor': prop[columns['floor']] if 'floor' in columns else '',
                'room_type': prop[columns['room_type']],
                'latitude': prop[columns['latitude']],
                'longitude': prop[columns['longitude']],
                'building_type': prop[columns['building_type']] if 'building_type' in columns else 'apartment',
                'renovation_status': prop[columns['renovation_status']] if 'renovation_status' in columns else 'unknown',
                'first_published_date': prop[columns['first_published_date']] if 'first_published_date' in columns else None,
                'upload_week': prop[columns['upload_week']] if 'upload_week' in columns else None,
                'status': prop[columns['status']] if 'status' in columns else 'active'
            }
            
            distance = haversine_distance(query_lat, query_lon, prop_dict['latitude'], prop_dict['longitude'])
            
            if distance_min <= distance <= distance_max:
                weeks_since = calculate_weeks_since_published(prop_dict['first_published_date'])
                prop_dict['weeks_since_first_published'] = weeks_since
                prop_dict['distance'] = distance
                
                if building_type and building_type != 'å…¨éƒ¨':
                    prop_building = prop_dict['building_type']
                    if building_type == 'å…¬å¯“' and prop_building != 'apartment':
                        continue
                    if building_type == 'é›»æ¢¯å¤§æ¨“' and prop_building != 'building':
                        continue
                
                if room_type and room_type != 'å…¨éƒ¨' and prop_dict['room_type'] != room_type:
                    continue
                
                filtered_properties.append(prop_dict)
        
        conn.close()
        
        active_properties = [p for p in filtered_properties if p['status'] == 'active']
        deleted_properties = [p for p in filtered_properties if p['status'] == 'deleted']
        
        if active_properties:
            avg_rent = sum(p['rent_monthly'] for p in active_properties) / len(active_properties)
            min_rent = min(p['rent_monthly'] for p in active_properties)
            max_rent = max(p['rent_monthly'] for p in active_properties)
            avg_area = sum(p['area'] for p in active_properties if p['area'] > 0) / max(1, len([p for p in active_properties if p['area'] > 0]))
        else:
            avg_rent = min_rent = max_rent = avg_area = 0
        
        room_type_counts = {}
        for p in active_properties:
            rt = p['room_type'] or 'æœªçŸ¥'
            room_type_counts[rt] = room_type_counts.get(rt, 0) + 1
        
        room_type_analysis = [{"room_type": rt, "count": count} for rt, count in sorted(room_type_counts.items(), key=lambda x: -x[1])]
        
        return {
            "status": "success",
            "query": {
                "address": address,
                "coordinates": {"latitude": query_lat, "longitude": query_lon},
                "distance_range": {"min": distance_min, "max": distance_max},
                "week_id": week_id or "current"
            },
            "summary": {
                "total_properties": len(filtered_properties),
                "active_properties": len(active_properties),
                "deleted_properties": len(deleted_properties),
                "new_properties": len([p for p in active_properties if p.get('weeks_since_first_published', 0) == 0]),
                "avg_rent_all": round(avg_rent),
                "min_rent": min_rent,
                "max_rent": max_rent,
                "avg_area": round(avg_area, 1)
            },
            "properties": filtered_properties,
            "room_type_analysis": room_type_analysis
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

class ResetRequest(BaseModel):
    password: str

@app.post("/api/admin/reset-database")
async def reset_database(request: ResetRequest):
    if request.password != "1234":
        raise HTTPException(status_code=403, detail="å¯†ç¢¼éŒ¯èª¤")
    
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM properties")
        cursor.execute("DELETE FROM versions")
        cursor.execute("DELETE FROM sqlite_sequence WHERE name='properties'")
        cursor.execute("DELETE FROM sqlite_sequence WHERE name='versions'")
        conn.commit()
        conn.close()
        auto_import_csv_files()
        return {"status": "success", "message": "æ•¸æ“šåº«å·²é‡ç½®ä¸¦é‡æ–°å°å…¥æ•¸æ“š"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/admin/database-status")
async def database_status():
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM properties")
        total = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM properties WHERE status = 'active'")
        active = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM properties WHERE status = 'deleted'")
        deleted = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM properties WHERE latitude != 0 AND longitude != 0")
        with_coords = cursor.fetchone()[0]
        
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            "status": "success",
            "database": {
                "total_properties": total,
                "active_properties": active,
                "deleted_properties": deleted,
                "with_coordinates": with_coords,
                "versions_count": len(versions),
                "versions": versions
            },
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

static_dir = os.path.dirname(__file__)
if os.path.exists(static_dir):
    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")
