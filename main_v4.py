"""
ç§Ÿå±‹è¡Œæƒ…åˆ†æç³»çµ± - ç‰ˆæœ¬æ§åˆ¶ API v6.0
æ”¯æŒå››è±¡é™åˆ†é¡ï¼ˆå»ºç‰©é¡å‹ x æˆ¿å‹å¤§é¡ï¼‰æŒ‰éœ€è¼‰å…¥ CSV
å„ªåŒ–æ•ˆèƒ½ï¼šåªè¼‰å…¥æŒ‡å®šç¯©é¸æ¢ä»¶çš„æ•¸æ“š
"""

import sqlite3
import json
import os
import re
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional
import math
import pandas as pd

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="ç§Ÿå±‹è¡Œæƒ…åˆ†æ API v6.0")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•¸æ“šåº«è·¯å¾‘
DB_PATH = os.path.join(os.path.dirname(__file__), "rental.db")

# Upload è³‡æ–™å¤¾è·¯å¾‘
UPLOAD_DIR = None

def get_upload_dir():
    global UPLOAD_DIR
    if UPLOAD_DIR:
        return UPLOAD_DIR
    
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "upload"),
        "/app/upload",
        "./upload",
        os.path.join(os.getcwd(), "upload")
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            UPLOAD_DIR = path
            return UPLOAD_DIR
    
    UPLOAD_DIR = possible_paths[0]
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)
    return UPLOAD_DIR

# ============ æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶ ============

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚åˆå§‹åŒ–æ•¸æ“šåº«ä¸¦æƒæå¯ç”¨çš„ CSV æ–‡ä»¶"""
    init_database()
    scan_available_csv_files()

# ============ æ•¸æ“šåº«åˆå§‹åŒ– ============

def init_database():
    """åˆå§‹åŒ–æ•¸æ“šåº«"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # ç‰ˆæœ¬è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS versions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            week_id TEXT UNIQUE NOT NULL,
            upload_date TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # CSV æ–‡ä»¶ç´¢å¼•è¡¨ï¼ˆè¨˜éŒ„å¯ç”¨çš„ CSV æ–‡ä»¶ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS csv_index (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            filename TEXT UNIQUE NOT NULL,
            city TEXT,
            district TEXT,
            building_type TEXT,
            property_category TEXT,
            week_id TEXT,
            record_count INTEGER DEFAULT 0,
            last_scanned TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    conn.commit()
    conn.close()

# ============ å·¥å…·å‡½æ•¸ ============

def get_week_id(date: datetime = None) -> str:
    if date is None:
        date = datetime.now()
    year = date.year % 100
    week = date.isocalendar()[1]
    return f"{year:02d}{week:02d}"

def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    R = 6371000
    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    delta_lat = math.radians(lat2 - lat1)
    delta_lon = math.radians(lon2 - lon1)
    a = math.sin(delta_lat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R * c

def calculate_weeks_since_published(first_published_date: str) -> int:
    if not first_published_date:
        return 0
    try:
        first_date = datetime.strptime(first_published_date, "%Y-%m-%d")
        now = datetime.now()
        delta = now - first_date
        weeks = delta.days // 7
        return max(0, weeks)
    except:
        return 0

def parse_dms_coordinate(coord_str: str):
    """è§£æåº¦åˆ†ç§’æ ¼å¼çš„åº§æ¨™å­—ä¸²"""
    if not coord_str or coord_str == 'nan':
        return 0, 0
    
    try:
        coord_str = str(coord_str).strip()
        pattern = r"(\d+)Â°(\d+)'(\d+(?:\.\d+)?)\"([NSEW])"
        matches = re.findall(pattern, coord_str)
        
        if len(matches) >= 2:
            lat_match = None
            lng_match = None
            
            for match in matches:
                deg, min_, sec, direction = match
                if direction in ['N', 'S']:
                    lat_match = (float(deg), float(min_), float(sec), direction)
                elif direction in ['E', 'W']:
                    lng_match = (float(deg), float(min_), float(sec), direction)
            
            if lat_match and lng_match:
                lat = lat_match[0] + lat_match[1]/60 + lat_match[2]/3600
                if lat_match[3] == 'S':
                    lat = -lat
                
                lng = lng_match[0] + lng_match[1]/60 + lng_match[2]/3600
                if lng_match[3] == 'W':
                    lng = -lng
                
                return round(lat, 6), round(lng, 6)
        
        return 0, 0
    except Exception as e:
        print(f"åº§æ¨™è§£æéŒ¯èª¤: {coord_str} - {e}")
        return 0, 0

def parse_csv_filename(filename: str) -> dict:
    """
    è§£æ CSV æ–‡ä»¶åï¼Œæå–åˆ†é¡ä¿¡æ¯
    æ”¯æ´æ ¼å¼ï¼š
    - æ–°æ ¼å¼: æ–°åŒ—å¸‚_ä¸­å’Œå€_å…¬å¯“_å¥—æˆ¿_2604.csv
    - èˆŠæ ¼å¼: 591_ä¸­å’Œå€_å…¬å¯“_æ•´å±¤ä½å®¶_page1.csv
    - åˆä½µæ ¼å¼: ä¸­å’Œå…¬å¯“å¥—æˆ¿_2603_merged.csv
    """
    result = {
        'city': '',
        'district': '',
        'building_type': '',  # apartment æˆ– building
        'property_category': '',  # å¥—æˆ¿ æˆ– ä½å®¶
        'week_id': ''
    }
    
    # ç§»é™¤ .csv å¾Œç¶´
    name = filename.replace('.csv', '')
    
    # å˜—è©¦æå–é€±æ¬¡
    week_match = re.search(r'_(\d{4})(?:_merged)?$', name)
    if week_match:
        result['week_id'] = week_match.group(1)
    
    # æå–å»ºç¯‰é¡å‹
    if 'é›»æ¢¯å¤§æ¨“' in filename or 'é›»æ¢¯' in filename:
        result['building_type'] = 'building'
    elif 'å…¬å¯“' in filename:
        result['building_type'] = 'apartment'
    
    # æå–æˆ¿å‹å¤§é¡
    if 'å¥—æˆ¿' in filename or 'ç¨ç«‹å¥—æˆ¿' in filename:
        result['property_category'] = 'å¥—æˆ¿'
    elif 'ä½å®¶' in filename or 'æ•´å±¤ä½å®¶' in filename:
        result['property_category'] = 'ä½å®¶'
    
    # æå–å€åŸŸ
    districts = [
        'æ¿æ©‹å€', 'ä¸‰é‡å€', 'ä¸­å’Œå€', 'æ°¸å’Œå€', 'æ–°èŠå€', 'æ–°åº—å€', 'åœŸåŸå€',
        'è˜†æ´²å€', 'æ¨¹æ—å€', 'æ±æ­¢å€', 'é¶¯æ­Œå€', 'ä¸‰å³½å€', 'æ·¡æ°´å€', 'ç‘èŠ³å€',
        'äº”è‚¡å€', 'æ³°å±±å€', 'æ—å£å€', 'æ·±å‘å€', 'çŸ³ç¢‡å€', 'åªæ—å€', 'ä¸‰èŠå€',
        'çŸ³é–€å€', 'å…«é‡Œå€', 'å¹³æºªå€', 'é›™æºªå€', 'è²¢å¯®å€', 'é‡‘å±±å€', 'è¬é‡Œå€',
        'çƒä¾†å€'
    ]
    
    for district in districts:
        if district in filename:
            result['district'] = district
            result['city'] = 'æ–°åŒ—å¸‚'
            break
    
    # å¦‚æœæ–‡ä»¶åä»¥åŸå¸‚é–‹é ­
    if filename.startswith('æ–°åŒ—å¸‚'):
        result['city'] = 'æ–°åŒ—å¸‚'
    elif filename.startswith('è‡ºåŒ—å¸‚') or filename.startswith('å°åŒ—å¸‚'):
        result['city'] = 'è‡ºåŒ—å¸‚'
    elif filename.startswith('åŸºéš†å¸‚'):
        result['city'] = 'åŸºéš†å¸‚'
    elif filename.startswith('æ¡ƒåœ’å¸‚'):
        result['city'] = 'æ¡ƒåœ’å¸‚'
    
    return result

def scan_available_csv_files():
    """æƒæ upload è³‡æ–™å¤¾ä¸­çš„ CSV æ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•"""
    upload_dir = get_upload_dir()
    
    if not os.path.exists(upload_dir):
        print(f"âš ï¸ Upload è³‡æ–™å¤¾ä¸å­˜åœ¨: {upload_dir}")
        return
    
    csv_files = [f for f in os.listdir(upload_dir) if f.endswith('.csv')]
    
    if not csv_files:
        print("âš ï¸ upload è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
        return
    
    print(f"ğŸ“ æƒæåˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆ")
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # æ¸…ç©ºèˆŠç´¢å¼•
    cursor.execute("DELETE FROM csv_index")
    
    week_ids = set()
    
    for csv_filename in csv_files:
        try:
            info = parse_csv_filename(csv_filename)
            
            # è¨ˆç®—è¨˜éŒ„æ•¸
            csv_path = os.path.join(upload_dir, csv_filename)
            try:
                df = pd.read_csv(csv_path, encoding='utf-8-sig', nrows=0)
                record_count = sum(1 for _ in open(csv_path, encoding='utf-8-sig')) - 1
            except:
                record_count = 0
            
            cursor.execute("""
                INSERT OR REPLACE INTO csv_index 
                (filename, city, district, building_type, property_category, week_id, record_count, last_scanned)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (csv_filename, info['city'], info['district'], info['building_type'], 
                  info['property_category'], info['week_id'], record_count, datetime.now().isoformat()))
            
            if info['week_id']:
                week_ids.add(info['week_id'])
            
            print(f"  âœ“ {csv_filename}: {info['district']} / {info['building_type']} / {info['property_category']} / {info['week_id']}")
        
        except Exception as e:
            print(f"  âš ï¸ {csv_filename} è§£æå¤±æ•—: {e}")
    
    # æ›´æ–°ç‰ˆæœ¬è¡¨
    upload_date = datetime.now().strftime("%Y-%m-%d")
    for week_id in week_ids:
        cursor.execute("INSERT OR REPLACE INTO versions (week_id, upload_date) VALUES (?, ?)", (week_id, upload_date))
    
    conn.commit()
    conn.close()
    
    print(f"âœ… CSV ç´¢å¼•å»ºç«‹å®Œæˆï¼Œé€±æ¬¡ç‰ˆæœ¬: {', '.join(sorted(week_ids))}")

def load_csv_data(city: str, district: str, building_type: str = None, property_category: str = None, week_id: str = None) -> list:
    """
    æŒ‰éœ€è¼‰å…¥æŒ‡å®šæ¢ä»¶çš„ CSV æ•¸æ“š
    
    åƒæ•¸:
    - city: ç¸£å¸‚
    - district: å€åŸŸ
    - building_type: å»ºç¯‰é¡å‹ (apartment/building/None=å…¨éƒ¨)
    - property_category: æˆ¿å‹å¤§é¡ (å¥—æˆ¿/ä½å®¶/None=å…¨éƒ¨)
    - week_id: é€±æ¬¡
    
    è¿”å›: æˆ¿æºåˆ—è¡¨
    """
    upload_dir = get_upload_dir()
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # æ§‹å»ºæŸ¥è©¢æ¢ä»¶
    query = "SELECT filename FROM csv_index WHERE 1=1"
    params = []
    
    if district:
        query += " AND district = ?"
        params.append(district)
    
    if building_type and building_type != 'å…¨éƒ¨':
        bt = 'apartment' if building_type == 'å…¬å¯“' else 'building' if building_type == 'é›»æ¢¯å¤§æ¨“' else building_type
        query += " AND building_type = ?"
        params.append(bt)
    
    if property_category and property_category != 'å…¨éƒ¨':
        query += " AND property_category = ?"
        params.append(property_category)
    
    if week_id:
        query += " AND week_id = ?"
        params.append(week_id)
    
    cursor.execute(query, params)
    csv_files = [row[0] for row in cursor.fetchall()]
    conn.close()
    
    print(f"ğŸ“‚ è¼‰å…¥ CSV: district={district}, building={building_type}, category={property_category}, week={week_id}")
    print(f"   æ‰¾åˆ° {len(csv_files)} å€‹åŒ¹é…çš„ CSV æ–‡ä»¶: {csv_files}")
    
    all_properties = []
    
    for csv_filename in csv_files:
        try:
            csv_path = os.path.join(upload_dir, csv_filename)
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            
            # å¾æ–‡ä»¶åæå–ä¿¡æ¯
            file_info = parse_csv_filename(csv_filename)
            
            for _, row in df.iterrows():
                # æå–æ¡ˆä»¶ç·¨è™Ÿ
                property_id = row.get('æ¡ˆä»¶ç·¨è™Ÿ', '')
                if pd.isna(property_id) or not property_id:
                    continue
                property_id = str(int(property_id) if isinstance(property_id, float) else property_id)
                
                # æå–æ¨™é¡Œ
                title = str(row.get('æ¨™é¡Œ', ''))
                
                # æå–åœ°å€
                raw_address = str(row.get('åœ°å€', ''))
                # è£œå……åŸå¸‚å’Œå€åŸŸ
                if file_info['city'] and not raw_address.startswith(file_info['city']):
                    raw_address = file_info['city'] + raw_address
                if file_info['district'] and file_info['district'] not in raw_address:
                    raw_address = raw_address.replace(file_info['city'], file_info['city'] + file_info['district'])
                address = raw_address
                
                # ç§Ÿé‡‘
                rent = row.get('ç§Ÿé‡‘', 0)
                if pd.isna(rent):
                    rent = 0
                rent = int(rent)
                
                # åªæ•¸
                area = row.get('åªæ•¸', row.get('å¡æ•¸', 0))
                if pd.isna(area):
                    area = 0
                area = float(area)
                
                # æˆ¿å‹ï¼ˆç´°åˆ†ï¼‰
                room_type = str(row.get('æˆ¿å‹', ''))
                if room_type == 'nan':
                    room_type = ''
                
                # æ¨“å±¤
                floor = str(row.get('æ¨“å±¤', ''))
                if floor == 'nan':
                    floor = ''
                
                # å»ºç¯‰é¡å‹
                building_type_val = file_info['building_type'] or 'unknown'
                
                # æˆ¿å‹å¤§é¡
                property_category_val = file_info['property_category'] or ''
                
                # åº§æ¨™è™•ç†
                latitude = 0
                longitude = 0
                
                if 'ç·¯åº¦' in df.columns and 'ç¶“åº¦' in df.columns:
                    lat_val = row.get('ç·¯åº¦', 0)
                    lng_val = row.get('ç¶“åº¦', 0)
                    if not pd.isna(lat_val) and not pd.isna(lng_val):
                        latitude = float(lat_val)
                        longitude = float(lng_val)
                
                if latitude == 0 and longitude == 0 and 'åº§æ¨™' in df.columns:
                    coord_str = row.get('åº§æ¨™', '')
                    if not pd.isna(coord_str):
                        latitude, longitude = parse_dms_coordinate(str(coord_str))
                
                # é€±æ¬¡
                prop_week_id = row.get('é€±æ¬¡', row.get('å¹´é€±', ''))
                if pd.isna(prop_week_id) or not prop_week_id:
                    prop_week_id = file_info['week_id'] or get_week_id()
                prop_week_id = str(prop_week_id)
                if prop_week_id.endswith('.0'):
                    prop_week_id = prop_week_id[:-2]
                
                # è·³éç„¡æ•ˆæ•¸æ“š
                if not address or rent <= 0:
                    continue
                
                all_properties.append({
                    'property_id': property_id,
                    'title': title,
                    'address': address,
                    'rent_monthly': rent,
                    'area': area,
                    'room_type': room_type,
                    'floor': floor,
                    'latitude': latitude,
                    'longitude': longitude,
                    'building_type': building_type_val,
                    'property_category': property_category_val,
                    'upload_week': prop_week_id,
                    'status': 'active'
                })
        
        except Exception as e:
            print(f"  âš ï¸ {csv_filename} è®€å–å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"   è¼‰å…¥å®Œæˆ: {len(all_properties)} ç­†æˆ¿æº")
    return all_properties

# ============ API ç«¯é» ============

@app.get("/api/versions")
async def get_versions():
    """ç²å–æ‰€æœ‰å¯ç”¨çš„é€±æ¬¡ç‰ˆæœ¬"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        conn.close()
        return {"status": "success", "versions": versions, "count": len(versions)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/available-filters")
async def get_available_filters():
    """ç²å–å¯ç”¨çš„ç¯©é¸é¸é …ï¼ˆåŸºæ–¼ç¾æœ‰ CSV æ–‡ä»¶ï¼‰"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # ç²å–å¯ç”¨çš„å€åŸŸ
        cursor.execute("SELECT DISTINCT city, district FROM csv_index WHERE district != '' ORDER BY city, district")
        districts = [{"city": row[0], "district": row[1]} for row in cursor.fetchall()]
        
        # ç²å–å¯ç”¨çš„å»ºç¯‰é¡å‹
        cursor.execute("SELECT DISTINCT building_type FROM csv_index WHERE building_type != ''")
        building_types = [row[0] for row in cursor.fetchall()]
        
        # ç²å–å¯ç”¨çš„æˆ¿å‹å¤§é¡
        cursor.execute("SELECT DISTINCT property_category FROM csv_index WHERE property_category != ''")
        property_categories = [row[0] for row in cursor.fetchall()]
        
        # ç²å–å¯ç”¨çš„é€±æ¬¡
        cursor.execute("SELECT DISTINCT week_id FROM csv_index WHERE week_id != '' ORDER BY week_id DESC")
        week_ids = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            "status": "success",
            "filters": {
                "districts": districts,
                "building_types": building_types,
                "property_categories": property_categories,
                "week_ids": week_ids
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis_v4")
async def analysis_v4(
    address: str,
    district: Optional[str] = None,
    distance_min: int = 0,
    distance_max: int = 5000,
    building_type: Optional[str] = None,
    property_category: Optional[str] = None,
    room_type: Optional[str] = None,
    week_id: Optional[str] = None,
    lat: Optional[float] = None,
    lng: Optional[float] = None
):
    """
    åˆ†æ API - æŒ‰éœ€è¼‰å…¥æŒ‡å®šæ¢ä»¶çš„æ•¸æ“š
    
    æ–°å¢åƒæ•¸:
    - district: å€åŸŸï¼ˆç”¨æ–¼æ±ºå®šè¼‰å…¥å“ªäº› CSVï¼‰
    - property_category: æˆ¿å‹å¤§é¡ï¼ˆå¥—æˆ¿/ä½å®¶ï¼Œç”¨æ–¼æ±ºå®šè¼‰å…¥å“ªäº› CSVï¼‰
    - room_type: æˆ¿å‹ç´°åˆ†ï¼ˆå¥—æˆ¿/2æˆ¿/3æˆ¿/3æˆ¿ä»¥ä¸Šï¼Œç”¨æ–¼å‰ç«¯ç¯©é¸ï¼‰
    """
    try:
        # ç¢ºå®šæŸ¥è©¢åº§æ¨™
        if lat is not None and lng is not None and lat != 0 and lng != 0:
            query_lat, query_lon = lat, lng
        else:
            query_lat, query_lon = 25.0288, 121.4625
        
        # å¾åœ°å€æå–å€åŸŸï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if not district:
            districts = [
                'æ¿æ©‹å€', 'ä¸‰é‡å€', 'ä¸­å’Œå€', 'æ°¸å’Œå€', 'æ–°èŠå€', 'æ–°åº—å€', 'åœŸåŸå€',
                'è˜†æ´²å€', 'æ¨¹æ—å€', 'æ±æ­¢å€', 'é¶¯æ­Œå€', 'ä¸‰å³½å€', 'æ·¡æ°´å€', 'ç‘èŠ³å€',
                'äº”è‚¡å€', 'æ³°å±±å€', 'æ—å£å€', 'æ·±å‘å€', 'çŸ³ç¢‡å€', 'åªæ—å€', 'ä¸‰èŠå€',
                'çŸ³é–€å€', 'å…«é‡Œå€', 'å¹³æºªå€', 'é›™æºªå€', 'è²¢å¯®å€', 'é‡‘å±±å€', 'è¬é‡Œå€',
                'çƒä¾†å€'
            ]
            for d in districts:
                if d in address:
                    district = d
                    break
        
        # æ±ºå®šè¦è¼‰å…¥çš„æˆ¿å‹å¤§é¡
        # å¦‚æœ room_type æ˜¯ã€Œå¥—æˆ¿ã€ï¼Œåªè¼‰å…¥å¥—æˆ¿ CSV
        # å¦‚æœ room_type æ˜¯ã€Œ2æˆ¿ã€ã€Œ3æˆ¿ã€ã€Œ3æˆ¿ä»¥ä¸Šã€ï¼Œåªè¼‰å…¥ä½å®¶ CSV
        # å¦‚æœ room_type æ˜¯ã€Œå…¨éƒ¨ã€æˆ–æœªæŒ‡å®šï¼Œè¼‰å…¥å…¨éƒ¨
        load_category = None
        if room_type == 'å¥—æˆ¿':
            load_category = 'å¥—æˆ¿'
        elif room_type in ['2æˆ¿', '3æˆ¿', '3æˆ¿ä»¥ä¸Š']:
            load_category = 'ä½å®¶'
        elif property_category:
            load_category = property_category
        
        # æŒ‰éœ€è¼‰å…¥ CSV æ•¸æ“š
        all_properties = load_csv_data(
            city='æ–°åŒ—å¸‚',
            district=district,
            building_type=building_type,
            property_category=load_category,
            week_id=week_id
        )
        
        # ç¯©é¸ç¬¦åˆæ¢ä»¶çš„æˆ¿æº
        filtered_properties = []
        for prop in all_properties:
            # æª¢æŸ¥åº§æ¨™
            if prop['latitude'] == 0 and prop['longitude'] == 0:
                continue
            
            # è¨ˆç®—è·é›¢
            distance = haversine_distance(query_lat, query_lon, prop['latitude'], prop['longitude'])
            
            # è·é›¢ç¯©é¸
            if distance_min <= distance <= distance_max:
                prop['distance'] = distance
                
                # æˆ¿å‹ç´°åˆ†ç¯©é¸ï¼ˆå‰ç«¯ç¯©é¸ï¼‰
                if room_type and room_type != 'å…¨éƒ¨':
                    if room_type == 'å¥—æˆ¿':
                        # å¥—æˆ¿ï¼šåªé¡¯ç¤ºå¥—æˆ¿
                        if prop.get('property_category') != 'å¥—æˆ¿' and 'å¥—æˆ¿' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '2æˆ¿':
                        if '2' not in prop.get('room_type', '') and 'å…©' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '3æˆ¿':
                        if '3' not in prop.get('room_type', '') and 'ä¸‰' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '3æˆ¿ä»¥ä¸Š':
                        rt = prop.get('room_type', '')
                        # æª¢æŸ¥æ˜¯å¦æœ‰ 4æˆ¿ä»¥ä¸Š
                        has_large = any(str(n) in rt for n in range(4, 10)) or any(c in rt for c in ['å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹'])
                        if not has_large:
                            continue
                
                filtered_properties.append(prop)
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        active_properties = [p for p in filtered_properties if p['status'] == 'active']
        
        if active_properties:
            avg_rent = sum(p['rent_monthly'] for p in active_properties) / len(active_properties)
            min_rent = min(p['rent_monthly'] for p in active_properties)
            max_rent = max(p['rent_monthly'] for p in active_properties)
            avg_area = sum(p['area'] for p in active_properties if p['area'] > 0) / max(1, len([p for p in active_properties if p['area'] > 0]))
        else:
            avg_rent = min_rent = max_rent = avg_area = 0
        
        # æˆ¿å‹åˆ†å¸ƒçµ±è¨ˆ
        room_type_counts = {}
        for p in active_properties:
            rt = p['room_type'] or 'æœªçŸ¥'
            room_type_counts[rt] = room_type_counts.get(rt, 0) + 1
        
        room_type_analysis = [{"room_type": rt, "count": count} for rt, count in sorted(room_type_counts.items(), key=lambda x: -x[1])]
        
        return {
            "status": "success",
            "query": {
                "address": address,
                "district": district,
                "coordinates": {"latitude": query_lat, "longitude": query_lon},
                "distance_range": {"min": distance_min, "max": distance_max},
                "building_type": building_type,
                "property_category": load_category,
                "room_type": room_type,
                "week_id": week_id or "current"
            },
            "summary": {
                "total_properties": len(filtered_properties),
                "active_properties": len(active_properties),
                "deleted_properties": len(filtered_properties) - len(active_properties),
                "new_properties": 0,
                "avg_rent_all": round(avg_rent),
                "min_rent": min_rent,
                "max_rent": max_rent,
                "avg_area": round(avg_area, 1)
            },
            "properties": filtered_properties,
            "room_type_analysis": room_type_analysis
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

class ResetRequest(BaseModel):
    password: str

@app.post("/api/admin/reset-database")
async def reset_database(request: ResetRequest):
    """é‡ç½®æ•¸æ“šåº«ä¸¦é‡æ–°æƒæ CSV"""
    if request.password != "1234":
        raise HTTPException(status_code=403, detail="å¯†ç¢¼éŒ¯èª¤")
    
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM csv_index")
        cursor.execute("DELETE FROM versions")
        conn.commit()
        conn.close()
        scan_available_csv_files()
        return {"status": "success", "message": "æ•¸æ“šåº«å·²é‡ç½®ä¸¦é‡æ–°æƒæ CSV æ–‡ä»¶"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/admin/database-status")
async def database_status():
    """ç²å–æ•¸æ“šåº«ç‹€æ…‹"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # CSV æ–‡ä»¶çµ±è¨ˆ
        cursor.execute("SELECT COUNT(*) FROM csv_index")
        csv_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT SUM(record_count) FROM csv_index")
        total_records = cursor.fetchone()[0] or 0
        
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        
        # CSV æ–‡ä»¶è©³æƒ…
        cursor.execute("SELECT filename, district, building_type, property_category, week_id, record_count FROM csv_index ORDER BY district, building_type, property_category")
        csv_files = [{"filename": row[0], "district": row[1], "building_type": row[2], "property_category": row[3], "week_id": row[4], "record_count": row[5]} for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            "status": "success",
            "database": {
                "csv_files_count": csv_count,
                "total_records": total_records,
                "versions_count": len(versions),
                "versions": versions,
                "csv_files": csv_files
            },
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/admin/rescan-csv")
async def rescan_csv():
    """é‡æ–°æƒæ CSV æ–‡ä»¶"""
    try:
        scan_available_csv_files()
        return {"status": "success", "message": "CSV æ–‡ä»¶å·²é‡æ–°æƒæ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# éœæ…‹æ–‡ä»¶æœå‹™
static_dir = os.path.dirname(__file__)
if os.path.exists(static_dir):
    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")
