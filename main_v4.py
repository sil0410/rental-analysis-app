"""
ç§Ÿå±‹è¡Œæƒ…åˆ†æç³»çµ± - ç‰ˆæœ¬æ§åˆ¶ API v8.1 (Fixed)
ä¿®æ­£ï¼šè‡ªå‹•é–å®šæœ€æ–°é€±æ¬¡ï¼Œè§£æ±ºæŸ¥è©¢å¡ä½å•é¡Œ
æ”¯æŒå››è±¡é™åˆ†é¡ï¼ˆå»ºç‰©é¡å‹ x æˆ¿å‹å¤§é¡ï¼‰æŒ‰éœ€è¼‰å…¥ CSV
æ”¯æŒ Google Drive åˆ†å±¤è³‡æ–™å¤¾ç®¡ç†
æ”¯æŒæœ¬åœ°å¿«å–æ©Ÿåˆ¶
"""

import sqlite3
import json
import os
import re
import time
import hashlib
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional
import math
import pandas as pd
from io import BytesIO

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="ç§Ÿå±‹è¡Œæƒ…åˆ†æ API v8.1 (Fixed)")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•¸æ“šåº«è·¯å¾‘
DB_PATH = os.path.join(os.path.dirname(__file__), "rental.db")

# Upload è³‡æ–™å¤¾è·¯å¾‘
UPLOAD_DIR = None

# ============ å¿«å–é…ç½® ============
CACHE_DIR = os.path.join(os.path.dirname(__file__), "csv_cache")
CACHE_EXPIRY_HOURS = 24  # å¿«å–éæœŸæ™‚é–“ï¼ˆå°æ™‚ï¼‰

def get_cache_path(file_id: str) -> str:
    """æ ¹æ“š file_id ç”Ÿæˆå¿«å–æª”æ¡ˆè·¯å¾‘"""
    return os.path.join(CACHE_DIR, f"{file_id}.csv")

def is_cache_valid(cache_path: str) -> bool:
    """æª¢æŸ¥å¿«å–æ˜¯å¦æœ‰æ•ˆï¼ˆå­˜åœ¨ä¸”æœªéæœŸï¼‰"""
    if not os.path.exists(cache_path):
        return False
    
    # æª¢æŸ¥å¿«å–æ˜¯å¦éæœŸ
    file_mtime = os.path.getmtime(cache_path)
    age_hours = (time.time() - file_mtime) / 3600
    return age_hours < CACHE_EXPIRY_HOURS

def clear_cache():
    """æ¸…é™¤æ‰€æœ‰å¿«å–æª”æ¡ˆ"""
    if os.path.exists(CACHE_DIR):
        import shutil
        shutil.rmtree(CACHE_DIR)
        os.makedirs(CACHE_DIR, exist_ok=True)
        print(f"âœ“ å¿«å–å·²æ¸…é™¤")
        return True
    return False

def get_cache_stats() -> dict:
    """ç²å–å¿«å–çµ±è¨ˆè³‡è¨Š"""
    if not os.path.exists(CACHE_DIR):
        return {"total_files": 0, "total_size_mb": 0, "oldest_file": None, "newest_file": None}
    
    files = [f for f in os.listdir(CACHE_DIR) if f.endswith('.csv')]
    total_size = sum(os.path.getsize(os.path.join(CACHE_DIR, f)) for f in files)
    
    if not files:
        return {"total_files": 0, "total_size_mb": 0, "oldest_file": None, "newest_file": None}
    
    file_times = [(f, os.path.getmtime(os.path.join(CACHE_DIR, f))) for f in files]
    file_times.sort(key=lambda x: x[1])
    
    return {
        "total_files": len(files),
        "total_size_mb": round(total_size / (1024 * 1024), 2),
        "oldest_file": {
            "name": file_times[0][0],
            "age_hours": round((time.time() - file_times[0][1]) / 3600, 1)
        },
        "newest_file": {
            "name": file_times[-1][0],
            "age_hours": round((time.time() - file_times[-1][1]) / 3600, 1)
        }
    }

# ============ Google Drive é…ç½® ============
GOOGLE_DRIVE_FOLDER_NAME = "ç§Ÿå±‹æ•¸æ“š"
drive_service = None
drive_folder_id = None
drive_available = False

def init_google_drive():
    """åˆå§‹åŒ– Google Drive APIï¼ˆå¯é¸åŠŸèƒ½ï¼‰"""
    global drive_service, drive_folder_id, drive_available
    
    try:
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– Google Drive é‡‘é‘°
        key_json_str = os.getenv('GOOGLE_DRIVE_KEY_JSON')
        
        if not key_json_str:
            print("â„¹ï¸ Google Drive æœªé…ç½®ï¼ˆç’°å¢ƒè®Šæ•¸ GOOGLE_DRIVE_KEY_JSON æœªè¨­å®šï¼‰")
            print("   ç³»çµ±å°‡ä½¿ç”¨æœ¬åœ° upload è³‡æ–™å¤¾")
            return False
        
        # å»¶é²å°å…¥ Google Drive ç›¸é—œæ¨¡çµ„
        try:
            from google.oauth2.service_account import Credentials
            from googleapiclient.discovery import build
        except ImportError:
            print("âš ï¸ Google Drive API æ¨¡çµ„æœªå®‰è£")
            print("   è«‹åŸ·è¡Œ: pip install google-auth google-api-python-client")
            return False
        
        # è§£æ JSON é‡‘é‘°
        try:
            key_dict = json.loads(key_json_str)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ ç„¡æ³•è§£æ Google Drive é‡‘é‘° JSONï¼š{e}")
            return False
        
        # å»ºç«‹èªè­‰
        credentials = Credentials.from_service_account_info(
            key_dict,
            scopes=['https://www.googleapis.com/auth/drive']
        )
        
        drive_service = build('drive', 'v3', credentials=credentials)
        
        # æŸ¥æ‰¾ã€Œç§Ÿå±‹æ•¸æ“šã€è³‡æ–™å¤¾
        results = drive_service.files().list(
            q=f"name='{GOOGLE_DRIVE_FOLDER_NAME}' and mimeType='application/vnd.google-apps.folder' and trashed=false",
            spaces='drive',
            fields='files(id, name)',
            pageSize=1
        ).execute()
        
        files = results.get('files', [])
        if files:
            drive_folder_id = files[0]['id']
            drive_available = True
            print(f"âœ“ Google Drive é€£æ¥æˆåŠŸ")
            print(f"  - è³‡æ–™å¤¾: {GOOGLE_DRIVE_FOLDER_NAME}")
            print(f"  - ID: {drive_folder_id}")
            return True
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ° Google Drive ä¸­çš„ã€Œ{GOOGLE_DRIVE_FOLDER_NAME}ã€è³‡æ–™å¤¾")
            return False
            
    except Exception as e:
        print(f"âš ï¸ Google Drive åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        return False

def download_file_from_drive(file_id: str, filename: str) -> Optional[pd.DataFrame]:
    """å¾ Google Drive ä¸‹è¼‰å–®ä¸€æª”æ¡ˆï¼ˆå¸¶å¿«å–ï¼‰"""
    if not drive_available or not drive_service:
        print(f"  âš ï¸ Google Drive ä¸å¯ç”¨")
        return None
    
    # ç¢ºä¿å¿«å–ç›®éŒ„å­˜åœ¨
    os.makedirs(CACHE_DIR, exist_ok=True)
    
    # æª¢æŸ¥å¿«å–
    cache_path = get_cache_path(file_id)
    if is_cache_valid(cache_path):
        try:
            df = pd.read_csv(cache_path, encoding='utf-8-sig')
            print(f"  âœ“ å¾å¿«å–è¼‰å…¥: {filename} ({len(df)} ç­†)")
            return df
        except Exception as e:
            print(f"  âš ï¸ å¿«å–è®€å–å¤±æ•—: {e}ï¼Œå°‡é‡æ–°ä¸‹è¼‰")
    
    # å¾ Google Drive ä¸‹è¼‰
    try:
        from googleapiclient.http import MediaIoBaseDownload
        
        request = drive_service.files().get_media(fileId=file_id)
        file_content = BytesIO()
        downloader = MediaIoBaseDownload(file_content, request)
        
        done = False
        while not done:
            status, done = downloader.next_chunk()
        
        file_content.seek(0)
        
        # å„²å­˜åˆ°å¿«å–
        with open(cache_path, 'wb') as f:
            f.write(file_content.read())
        
        # é‡æ–°è®€å–ä¸¦è¿”å› DataFrame
        df = pd.read_csv(cache_path, encoding='utf-8-sig')
        print(f"  âœ“ å¾ Google Drive ä¸‹è¼‰ä¸¦å¿«å–: {filename} ({len(df)} ç­†)")
        return df
        
    except Exception as e:
        print(f"  âš ï¸ ä¸‹è¼‰ {filename} (file_id={file_id}) å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

def normalize_city_name(city: str) -> list:
    """æ¨™æº–åŒ–åŸå¸‚åç¨±ï¼Œè¿”å›æ‰€æœ‰å¯èƒ½çš„è®Šé«”"""
    if not city:
        return []
    
    # å°åŒ—å¸‚çš„è®Šé«”
    taipei_variants = ['å°åŒ—å¸‚', 'è‡ºåŒ—å¸‚']
    if city in taipei_variants:
        return taipei_variants
    
    # å…¶ä»–åŸå¸‚ç›´æ¥è¿”å›
    return [city]

def get_csv_from_drive(city: str, district: str, building_type: str, property_category: str, week_id: str) -> Optional[pd.DataFrame]:
    """å¾ Google Drive è®€å–æŒ‡å®šçš„ CSV æ–‡ä»¶ï¼ˆä½¿ç”¨æ•¸æ“šåº«ä¸­çš„ file_idï¼Œå¸¶å¿«å–ï¼‰"""
    if not drive_available or not drive_service:
        print(f"  âš ï¸ Google Drive ä¸å¯ç”¨")
        return None
    
    try:
        # å¾æ•¸æ“šåº«æŸ¥è©¢åŒ¹é…çš„æª”æ¡ˆ
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # è½‰æ›å»ºç‰©é¡å‹æ ¼å¼
        bt_db = building_type
        if building_type == 'å…¬å¯“':
            bt_db = 'apartment'
        elif building_type == 'é›»æ¢¯å¤§æ¨“':
            bt_db = 'building'
        
        # ç²å–åŸå¸‚åç¨±çš„æ‰€æœ‰è®Šé«”
        city_variants = normalize_city_name(city)
        
        # æŸ¥è©¢åŒ¹é…çš„æª”æ¡ˆï¼ˆä½¿ç”¨ file_idï¼Œæ”¯æ´åŸå¸‚åç¨±è®Šé«”ï¼‰
        if city_variants:
            placeholders = ','.join(['?' for _ in city_variants])
            query = f"""
                SELECT filename, file_id FROM csv_index 
                WHERE city IN ({placeholders}) AND district = ? AND week_id = ? 
                AND source = 'google_drive' AND file_id IS NOT NULL
            """
            params = city_variants + [district, week_id]
        else:
            query = """
                SELECT filename, file_id FROM csv_index 
                WHERE district = ? AND week_id = ? 
                AND source = 'google_drive' AND file_id IS NOT NULL
            """
            params = [district, week_id]
        
        # å¦‚æœæŒ‡å®šäº†å»ºç‰©é¡å‹ï¼ŒåŠ å…¥ç¯©é¸æ¢ä»¶
        if bt_db and bt_db not in ['all', 'å…¨éƒ¨']:
            query += " AND building_type = ?"
            params.append(bt_db)
        
        # å¦‚æœæŒ‡å®šäº†æˆ¿å‹ï¼ŒåŠ å…¥ç¯©é¸æ¢ä»¶
        if property_category and property_category not in ['all', 'å…¨éƒ¨']:
            query += " AND property_category = ?"
            params.append(property_category)
        
        cursor.execute(query, params)
        results = cursor.fetchall()
        conn.close()
        
        print(f"  ğŸ“‚ æŸ¥è©¢ Google Drive: city={city}, district={district}, bt={bt_db}, cat={property_category}, week={week_id}")
        print(f"     æ‰¾åˆ° {len(results)} å€‹åŒ¹é…çš„æª”æ¡ˆ")
        
        if not results:
            return None
        
        # åˆä½µæ‰€æœ‰åŒ¹é…çš„ CSV æª”æ¡ˆï¼ˆä½¿ç”¨å¿«å–æ©Ÿåˆ¶ï¼‰
        all_dfs = []
        for filename, file_id in results:
            df = download_file_from_drive(file_id, filename)
            if df is not None:
                all_dfs.append(df)
        
        if not all_dfs:
            return None
        
        # åˆä½µæ‰€æœ‰ DataFrame
        combined_df = pd.concat(all_dfs, ignore_index=True)
        print(f"  âœ“ åˆä½µå®Œæˆ: å…± {len(combined_df)} ç­†è³‡æ–™")
        return combined_df
        
    except Exception as e:
        print(f"  âš ï¸ å¾ Google Drive è®€å– CSV å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        return None

def list_google_drive_files(folder_id: str, path: str = "") -> list:
    """éè¿´åˆ—å‡º Google Drive è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ CSV æª”æ¡ˆ"""
    if not drive_available or not drive_service:
        return []
    
    files_found = []
    
    try:
        # åˆ—å‡ºè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰é …ç›®
        results = drive_service.files().list(
            q=f"'{folder_id}' in parents and trashed=false",
            spaces='drive',
            fields='files(id, name, mimeType)',
            pageSize=1000
        ).execute()
        
        items = results.get('files', [])
        
        for item in items:
            item_name = item['name']
            item_id = item['id']
            item_type = item['mimeType']
            current_path = f"{path}/{item_name}" if path else item_name
            
            if item_type == 'application/vnd.google-apps.folder':
                # éè¿´é€²å…¥å­è³‡æ–™å¤¾
                sub_files = list_google_drive_files(item_id, current_path)
                files_found.extend(sub_files)
            elif item_name.endswith('.csv'):
                # æ‰¾åˆ° CSV æª”æ¡ˆ
                files_found.append({
                    'id': item_id,
                    'name': item_name,
                    'path': current_path
                })
        
        return files_found
        
    except Exception as e:
        print(f"âš ï¸ åˆ—å‡º Google Drive è³‡æ–™å¤¾å¤±æ•— ({path}): {e}")
        return []

# ============ æœ¬åœ°æ–‡ä»¶ç³»çµ± ============

def get_upload_dir():
    global UPLOAD_DIR
    if UPLOAD_DIR:
        return UPLOAD_DIR
    
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "upload"),
        "/app/upload",
        "./upload",
        os.path.join(os.getcwd(), "upload")
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            UPLOAD_DIR = path
            return UPLOAD_DIR
    
    UPLOAD_DIR = possible_paths[0]
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)
    return UPLOAD_DIR

# ============ æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶ ============

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚åˆå§‹åŒ–æ•¸æ“šåº«ä¸¦æƒæå¯ç”¨çš„ CSV æ–‡ä»¶"""
    try:
        # ç¢ºä¿å¿«å–ç›®éŒ„å­˜åœ¨
        os.makedirs(CACHE_DIR, exist_ok=True)
        
        init_database()
        init_google_drive()  # å˜—è©¦åˆå§‹åŒ– Google Driveï¼ˆå¯é¸ï¼‰
        scan_available_csv_files()
        
        # é¡¯ç¤ºå¿«å–ç‹€æ…‹
        cache_stats = get_cache_stats()
        print(f"ğŸ“¦ å¿«å–ç‹€æ…‹: {cache_stats['total_files']} å€‹æª”æ¡ˆ, {cache_stats['total_size_mb']} MB")
    except Exception as e:
        print(f"âš ï¸ å•Ÿå‹•äº‹ä»¶éŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()

# ============ æ•¸æ“šåº«åˆå§‹åŒ– ============

def init_database():
    """åˆå§‹åŒ–æ•¸æ“šåº«"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # ç‰ˆæœ¬è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS versions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            week_id TEXT UNIQUE NOT NULL,
            upload_date TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # CSV æ–‡ä»¶ç´¢å¼•è¡¨ï¼ˆè¨˜éŒ„å¯ç”¨çš„ CSV æ–‡ä»¶ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS csv_index (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            filename TEXT UNIQUE NOT NULL,
            city TEXT,
            district TEXT,
            building_type TEXT,
            property_category TEXT,
            week_id TEXT,
            record_count INTEGER DEFAULT 0,
            source TEXT DEFAULT 'local',
            file_id TEXT,
            last_scanned TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # å˜—è©¦æ–°å¢ file_id æ¬„ä½ï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ä½†æ²’æœ‰æ­¤æ¬„ä½ï¼‰
    try:
        cursor.execute("ALTER TABLE csv_index ADD COLUMN file_id TEXT")
    except:
        pass  # æ¬„ä½å·²å­˜åœ¨
    
    conn.commit()
    conn.close()

# ============ å·¥å…·å‡½æ•¸ ============

def get_week_id(date: datetime = None) -> str:
    if date is None:
        date = datetime.now()
    year = date.year % 100
    week = date.isocalendar()[1]
    return f"{year:02d}{week:02d}"

def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    R = 6371000
    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    delta_lat = math.radians(lat2 - lat1)
    delta_lon = math.radians(lon2 - lon1)
    a = math.sin(delta_lat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R * c

def calculate_weeks_since_published(first_published_date: str) -> int:
    if not first_published_date:
        return 0
    try:
        first_date = datetime.strptime(first_published_date, "%Y-%m-%d")
        now = datetime.now()
        delta = now - first_date
        weeks = delta.days // 7
        return max(0, weeks)
    except:
        return 0

def parse_dms_coordinate(coord_str: str):
    """è§£æåº¦åˆ†ç§’æ ¼å¼çš„åº§æ¨™å­—ä¸²"""
    if not coord_str or coord_str == 'nan':
        return 0, 0
    
    try:
        coord_str = str(coord_str).strip()
        pattern = r"(\d+)Â°(\d+)'(\d+(?:\.\d+)?)\"([NSEW])"
        matches = re.findall(pattern, coord_str)
        
        if len(matches) >= 2:
            lat_match = None
            lng_match = None
            
            for match in matches:
                deg, min_, sec, direction = match
                if direction in ['N', 'S']:
                    lat_match = match
                elif direction in ['E', 'W']:
                    lng_match = match
            
            if lat_match and lng_match:
                lat_deg, lat_min, lat_sec, lat_dir = lat_match
                lat = float(lat_deg) + float(lat_min)/60 + float(lat_sec)/3600
                if lat_dir == 'S':
                    lat = -lat
                
                lng_deg, lng_min, lng_sec, lng_dir = lng_match
                lng = float(lng_deg) + float(lng_min)/60 + float(lng_sec)/3600
                if lng_dir == 'W':
                    lng = -lng
                
                return lat, lng
    except Exception as e:
        pass
    
    return 0, 0

def parse_csv_filename(filename: str) -> dict:
    """è§£æ CSV æ–‡ä»¶åï¼Œæå–ç›¸é—œä¿¡æ¯"""
    result = {
        'city': '',
        'district': '',
        'building_type': '',
        'property_category': '',
        'week_id': ''
    }
    
    name = filename.replace('.csv', '')
    
    week_match = re.search(r'_(\d{4})(?:_merged)?$', name)
    if week_match:
        result['week_id'] = week_match.group(1)
    
    if 'é›»æ¢¯å¤§æ¨“' in filename or 'é›»æ¢¯' in filename:
        result['building_type'] = 'building'
    elif 'å…¬å¯“' in filename:
        result['building_type'] = 'apartment'
    
    if 'å¥—æˆ¿' in filename or 'ç¨ç«‹å¥—æˆ¿' in filename:
        result['property_category'] = 'å¥—æˆ¿'
    elif 'ä½å®¶' in filename or 'æ•´å±¤ä½å®¶' in filename:
        result['property_category'] = 'ä½å®¶'
    
    # å€åŸŸåç¨±å°ç…§è¡¨ï¼ˆåŒ…å«å¸¶ã€Œå€ã€å­—å’Œä¸å¸¶ã€Œå€ã€å­—çš„ç‰ˆæœ¬ï¼‰
    district_mapping = {
        # æ–°åŒ—å¸‚
        'æ¿æ©‹': 'æ¿æ©‹å€', 'æ¿æ©‹å€': 'æ¿æ©‹å€',
        'ä¸‰é‡': 'ä¸‰é‡å€', 'ä¸‰é‡å€': 'ä¸‰é‡å€',
        'ä¸­å’Œ': 'ä¸­å’Œå€', 'ä¸­å’Œå€': 'ä¸­å’Œå€',
        'æ°¸å’Œ': 'æ°¸å’Œå€', 'æ°¸å’Œå€': 'æ°¸å’Œå€',
        'æ–°èŠ': 'æ–°èŠå€', 'æ–°èŠå€': 'æ–°èŠå€',
        'æ–°åº—': 'æ–°åº—å€', 'æ–°åº—å€': 'æ–°åº—å€',
        'åœŸåŸ': 'åœŸåŸå€', 'åœŸåŸå€': 'åœŸåŸå€',
        'è˜†æ´²': 'è˜†æ´²å€', 'è˜†æ´²å€': 'è˜†æ´²å€',
        'æ¨¹æ—': 'æ¨¹æ—å€', 'æ¨¹æ—å€': 'æ¨¹æ—å€',
        'æ±æ­¢': 'æ±æ­¢å€', 'æ±æ­¢å€': 'æ±æ­¢å€',
        'é¶¯æ­Œ': 'é¶¯æ­Œå€', 'é¶¯æ­Œå€': 'é¶¯æ­Œå€',
        'ä¸‰å³½': 'ä¸‰å³½å€', 'ä¸‰å³½å€': 'ä¸‰å³½å€',
        'æ·¡æ°´': 'æ·¡æ°´å€', 'æ·¡æ°´å€': 'æ·¡æ°´å€',
        'äº”è‚¡': 'äº”è‚¡å€', 'äº”è‚¡å€': 'äº”è‚¡å€',
        'æ³°å±±': 'æ³°å±±å€', 'æ³°å±±å€': 'æ³°å±±å€',
        'æ—å£': 'æ—å£å€', 'æ—å£å€': 'æ—å£å€',
        'å…«é‡Œ': 'å…«é‡Œå€', 'å…«é‡Œå€': 'å…«é‡Œå€',
        # å°åŒ—å¸‚
        'å¤§å®‰': 'å¤§å®‰å€', 'å¤§å®‰å€': 'å¤§å®‰å€',
        'ä¿¡ç¾©': 'ä¿¡ç¾©å€', 'ä¿¡ç¾©å€': 'ä¿¡ç¾©å€',
        'ä¸­å±±': 'ä¸­å±±å€', 'ä¸­å±±å€': 'ä¸­å±±å€',
        'æ¾å±±': 'æ¾å±±å€', 'æ¾å±±å€': 'æ¾å±±å€',
        'å—æ¸¯': 'å—æ¸¯å€', 'å—æ¸¯å€': 'å—æ¸¯å€',
        'å…§æ¹–': 'å…§æ¹–å€', 'å…§æ¹–å€': 'å…§æ¹–å€',
        'åŒ—æŠ•': 'åŒ—æŠ•å€', 'åŒ—æŠ•å€': 'åŒ—æŠ•å€',
        'å£«æ—': 'å£«æ—å€', 'å£«æ—å€': 'å£«æ—å€',
        'å¤§åŒ': 'å¤§åŒå€', 'å¤§åŒå€': 'å¤§åŒå€',
        'ä¸­æ­£': 'ä¸­æ­£å€', 'ä¸­æ­£å€': 'ä¸­æ­£å€',
        'è¬è¯': 'è¬è¯å€', 'è¬è¯å€': 'è¬è¯å€',
        'æ–‡å±±': 'æ–‡å±±å€', 'æ–‡å±±å€': 'æ–‡å±±å€',
    }
    
    # å˜—è©¦åŒ¹é…å€åŸŸåç¨±ï¼ˆå„ªå…ˆåŒ¹é…è¼ƒé•·çš„åç¨±ï¼‰
    for short_name, full_name in sorted(district_mapping.items(), key=lambda x: len(x[0]), reverse=True):
        if short_name in filename:
            result['district'] = full_name
            # ä¸åœ¨é€™è£¡è¨­å®šåŸå¸‚ï¼Œè®“å¾ŒçºŒçš„è·¯å¾‘è§£æä¾†è¨­å®š
            break
    
    if filename.startswith('æ–°åŒ—å¸‚'):
        result['city'] = 'æ–°åŒ—å¸‚'
    elif filename.startswith('è‡ºåŒ—å¸‚') or filename.startswith('å°åŒ—å¸‚'):
        result['city'] = 'è‡ºåŒ—å¸‚'
    elif filename.startswith('åŸºéš†å¸‚'):
        result['city'] = 'åŸºéš†å¸‚'
    elif filename.startswith('æ¡ƒåœ’å¸‚'):
        result['city'] = 'æ¡ƒåœ’å¸‚'
    
    return result

def scan_available_csv_files():
    """æƒæ upload è³‡æ–™å¤¾å’Œ Google Drive ä¸­çš„ CSV æ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•"""
    upload_dir = get_upload_dir()
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # æ¸…ç©ºèˆŠç´¢å¼•
    cursor.execute("DELETE FROM csv_index")
    
    week_ids = set()
    total_files = 0
    
    # === æƒææœ¬åœ° upload è³‡æ–™å¤¾ ===
    if os.path.exists(upload_dir):
        csv_files = [f for f in os.listdir(upload_dir) if f.endswith('.csv')]
        print(f"ğŸ“ æœ¬åœ°æƒæåˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆ")
        
        for csv_filename in csv_files:
            try:
                info = parse_csv_filename(csv_filename)
                
                csv_path = os.path.join(upload_dir, csv_filename)
                try:
                    record_count = sum(1 for _ in open(csv_path, encoding='utf-8-sig')) - 1
                except:
                    record_count = 0
                
                cursor.execute("""
                    INSERT OR REPLACE INTO csv_index 
                    (filename, city, district, building_type, property_category, week_id, record_count, source, last_scanned)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (csv_filename, info['city'], info['district'], info['building_type'], 
                      info['property_category'], info['week_id'], record_count, 'local', datetime.now().isoformat()))
                
                if info['week_id']:
                    week_ids.add(info['week_id'])
                
                total_files += 1
                print(f"  âœ“ [local] {csv_filename}: {info['city']}/{info['district']} / {info['building_type']} / {info['property_category']} / {info['week_id']}")
            
            except Exception as e:
                print(f"  âš ï¸ {csv_filename} è™•ç†å¤±æ•—: {e}")
    else:
        print(f"âš ï¸ Upload è³‡æ–™å¤¾ä¸å­˜åœ¨: {upload_dir}")
    
    # === æƒæ Google Drive ä¸¦è‡ªå‹•ä¸‹è¼‰åˆ° upload è³‡æ–™å¤¾ ===
    print(f"ğŸ“ Google Drive ç‹€æ…‹: available={drive_available}, folder_id={drive_folder_id}")
    if drive_available and drive_folder_id:
        print(f"ğŸ“ é–‹å§‹æƒæ Google Drive ä¸¦åŒæ­¥æª”æ¡ˆ...")
        try:
            drive_files = list_google_drive_files(drive_folder_id)
            print(f"ğŸ“ Google Drive æƒæåˆ° {len(drive_files)} å€‹ CSV æª”æ¡ˆ")
            
            downloaded_count = 0
            skipped_count = 0
            
            for file_info in drive_files:
                try:
                    filename = file_info['name']
                    file_path = file_info['path']
                    file_id = file_info['id']
                    
                    # å¾è·¯å¾‘è§£æåŸå¸‚å’Œå€åŸŸ
                    # è·¯å¾‘æ ¼å¼: "ç¸£å¸‚/å€åŸŸ/æª”æ¡ˆå.csv"
                    path_parts = file_path.split('/')
                    city = ''
                    district = ''
                    
                    if len(path_parts) >= 3:
                        city = path_parts[0]
                        district = path_parts[1]
                    elif len(path_parts) == 2:
                        city = path_parts[0]
                    
                    info = parse_csv_filename(filename)
                    
                    # å¦‚æœå¾è·¯å¾‘è§£æåˆ°äº†åŸå¸‚å’Œå€åŸŸï¼Œå„ªå…ˆä½¿ç”¨è·¯å¾‘ä¸­çš„è³‡è¨Š
                    if city:
                        info['city'] = city
                    if district:
                        info['district'] = district
                    
                    # è‡ªå‹•ä¸‹è¼‰åˆ° upload è³‡æ–™å¤¾
                    local_path = os.path.join(upload_dir, filename)
                    record_count = 0
                    
                    if os.path.exists(local_path):
                        # æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰
                        try:
                            record_count = sum(1 for _ in open(local_path, encoding='utf-8-sig')) - 1
                        except:
                            record_count = 0
                        skipped_count += 1
                    else:
                        # ä¸‹è¼‰æª”æ¡ˆåˆ° upload è³‡æ–™å¤¾
                        try:
                            df = download_file_from_drive(file_id, filename)
                            if df is not None:
                                # å¾å¿«å–è¤‡è£½åˆ° upload è³‡æ–™å¤¾
                                cache_path = get_cache_path(file_id)
                                if os.path.exists(cache_path):
                                    import shutil
                                    shutil.copy2(cache_path, local_path)
                                    record_count = len(df)
                                    downloaded_count += 1
                                    print(f"  â¬‡ï¸ å·²ä¸‹è¼‰: {filename} ({record_count} ç­†)")
                                else:
                                    # å¦‚æœå¿«å–ä¸å­˜åœ¨ï¼Œç›´æ¥å„²å­˜ DataFrame
                                    df.to_csv(local_path, index=False, encoding='utf-8-sig')
                                    record_count = len(df)
                                    downloaded_count += 1
                                    print(f"  â¬‡ï¸ å·²ä¸‹è¼‰: {filename} ({record_count} ç­†)")
                        except Exception as download_error:
                            print(f"  âš ï¸ ä¸‹è¼‰å¤±æ•—: {filename} - {download_error}")
                    
                    # ç´¢å¼•è¨˜éŒ„ç‚ºæœ¬åœ°æª”æ¡ˆï¼ˆå› ç‚ºå·²ä¸‹è¼‰åˆ° uploadï¼‰
                    cursor.execute("""
                        INSERT OR REPLACE INTO csv_index 
                        (filename, city, district, building_type, property_category, week_id, record_count, source, file_id, last_scanned)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (filename, info['city'], info['district'], info['building_type'], 
                          info['property_category'], info['week_id'], record_count, 'local', file_id, datetime.now().isoformat()))
                    
                    if info['week_id']:
                        week_ids.add(info['week_id'])
                    
                    total_files += 1
                
                except Exception as e:
                    print(f"  âš ï¸ {file_info['name']} è™•ç†å¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
            
            print(f"âœ… Google Drive åŒæ­¥å®Œæˆ: æ–°ä¸‹è¼‰ {downloaded_count} å€‹, å·²å­˜åœ¨ {skipped_count} å€‹")
        except Exception as e:
            print(f"âš ï¸ Google Drive æƒæéç¨‹å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"â„¹ï¸ Google Drive æœªé…ç½®æˆ–ä¸å¯ç”¨ (available={drive_available}, folder_id={drive_folder_id})")
    
    # === æ›´æ–°ç‰ˆæœ¬è¨˜éŒ„ ===
    for week_id in week_ids:
        cursor.execute("""
            INSERT OR REPLACE INTO versions (week_id, upload_date)
            VALUES (?, ?)
        """, (week_id, datetime.now().strftime("%Y-%m-%d")))
    
    conn.commit()
    conn.close()
    
    print(f"âœ“ ç´¢å¼•å»ºç«‹å®Œæˆ: {total_files} å€‹æ–‡ä»¶, {len(week_ids)} å€‹é€±æ¬¡ç‰ˆæœ¬")

def load_csv_data(city: str, district: str, building_type: str, property_category: str, week_id: str) -> List[dict]:
    """
    æŒ‰éœ€è¼‰å…¥ CSV æ•¸æ“š
    å„ªå…ˆå¾ Google Drive è¼‰å…¥ï¼Œæ¬¡ä¹‹å¾æœ¬åœ° upload è³‡æ–™å¤¾
    """
    upload_dir = get_upload_dir()
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    query = "SELECT filename FROM csv_index WHERE 1=1"
    params = []
    
    if district:
        query += " AND district = ?"
        params.append(district)
    
    if building_type and building_type != 'å…¨éƒ¨':
        bt = 'apartment' if building_type == 'å…¬å¯“' else 'building' if building_type == 'é›»æ¢¯å¤§æ¨“' else building_type
        query += " AND building_type = ?"
        params.append(bt)
    
    if property_category and property_category != 'å…¨éƒ¨':
        query += " AND property_category = ?"
        params.append(property_category)
    
    if week_id:
        query += " AND week_id = ?"
        params.append(week_id)
    
    cursor.execute(query, params)
    csv_files = [row[0] for row in cursor.fetchall()]
    conn.close()
    
    print(f"ğŸ“‚ è¼‰å…¥ CSV: district={district}, building={building_type}, category={property_category}, week={week_id}")
    print(f"   æ‰¾åˆ° {len(csv_files)} å€‹åŒ¹é…çš„ CSV æ–‡ä»¶: {csv_files}")
    
    all_properties = []
    
    # å˜—è©¦å¾ Google Drive è¼‰å…¥ï¼ˆä½¿ç”¨å¿«å–ï¼‰
    if drive_available and district and week_id:
        print(f"ğŸ“‚ å˜—è©¦å¾ Google Drive è¼‰å…¥: city={city}, district={district}, week={week_id}")
        
        # ç›´æ¥ä½¿ç”¨ get_csv_from_driveï¼Œå®ƒæœƒè‡ªå‹•è™•ç†å»ºç‰©é¡å‹å’Œæˆ¿å‹çš„ç¯©é¸
        df = get_csv_from_drive(city, district, building_type, property_category, week_id)
        if df is not None:
            # å¾æ•¸æ“šåº«ç²å–å»ºç‰©é¡å‹å’Œæˆ¿å‹è³‡è¨Š
            properties = process_dataframe(df, city, district, building_type or 'å…¨éƒ¨', property_category or 'å…¨éƒ¨', week_id)
            all_properties.extend(properties)
            print(f"   âœ“ å¾ Google Drive è¼‰å…¥ {len(properties)} ç­†è³‡æ–™")
    
    # å¦‚æœ Google Drive æ²’æœ‰æ•¸æ“šï¼Œå¾æœ¬åœ°è¼‰å…¥
    if not all_properties:
        for csv_filename in csv_files:
            try:
                csv_path = os.path.join(upload_dir, csv_filename)
                df = pd.read_csv(csv_path, encoding='utf-8-sig')
                
                file_info = parse_csv_filename(csv_filename)
                
                properties = process_dataframe(
                    df, 
                    file_info['city'], 
                    file_info['district'], 
                    file_info['building_type'], 
                    file_info['property_category'], 
                    file_info['week_id']
                )
                all_properties.extend(properties)
            
            except Exception as e:
                print(f"  âš ï¸ {csv_filename} è®€å–å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
    
    print(f"   è¼‰å…¥å®Œæˆ: {len(all_properties)} ç­†æˆ¿æº")
    return all_properties

def get_all_week_ids() -> List[str]:
    """ç²å–æ‰€æœ‰å¯ç”¨çš„é€±æ¬¡ IDï¼ŒæŒ‰é™åºæ’åˆ—"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        # ä¿®æ­£ï¼šéæ¿¾ç©ºå€¼å’Œç„¡æ•ˆå€¼
        cursor.execute("SELECT DISTINCT week_id FROM csv_index WHERE week_id IS NOT NULL AND week_id != '' ORDER BY week_id DESC")
        week_ids = [row[0] for row in cursor.fetchall()]
        conn.close()
        return week_ids
    except:
        return []

def load_property_ids_for_week(city: str, district: str, building_type: str, property_category: str, week_id: str) -> set:
    """è¼‰å…¥æŒ‡å®šé€±æ¬¡çš„æ‰€æœ‰æ¡ˆä»¶ç·¨è™Ÿ"""
    properties = load_csv_data(city, district, building_type, property_category, week_id)
    return set(p['property_id'] for p in properties if p.get('property_id'))

def calculate_property_status(current_properties: List[dict], city: str, district: str, 
                               building_type: str, property_category: str, current_week_id: str) -> List[dict]:
    """
    è¨ˆç®—æ¯å€‹æˆ¿æºçš„ç‹€æ…‹ï¼ˆæ–°å¢/æŒçºŒ/æ¶ˆå¤±ï¼‰
    - æ–°å¢ï¼šæœ¬é€±é¦–æ¬¡å‡ºç¾ -> status='new', weeks_active=1
    - æŒçºŒï¼šå·²å­˜åœ¨å¤šé€± -> status='active', weeks_active=N
    - æ¶ˆå¤±ï¼šä¹‹å‰æœ‰ä½†æœ¬é€±æ²’æœ‰ -> status='inactive'
    """
    all_weeks = get_all_week_ids()
    
    if not all_weeks or current_week_id not in all_weeks:
        # æ²’æœ‰æ­·å²è³‡æ–™ï¼Œæ‰€æœ‰éƒ½æ˜¯æ–°å¢
        for prop in current_properties:
            prop['status'] = 'new'
            prop['weeks_active'] = 1
            prop['first_seen_week'] = current_week_id
        return current_properties
    
    current_week_index = all_weeks.index(current_week_id)
    
    # ç²å–æ­·å²é€±æ¬¡ï¼ˆæœ€å¤šå›æº¯ 10 é€±ï¼‰
    history_weeks = all_weeks[current_week_index + 1:current_week_index + 11]
    
    # è¼‰å…¥æ­·å²é€±æ¬¡çš„æ¡ˆä»¶ç·¨è™Ÿ
    history_property_ids = {}  # {week_id: set of property_ids}
    for week in history_weeks:
        try:
            ids = load_property_ids_for_week(city, district, building_type, property_category, week)
            history_property_ids[week] = ids
        except:
            history_property_ids[week] = set()
    
    # åˆä½µæ‰€æœ‰æ­·å²æ¡ˆä»¶ç·¨è™Ÿ
    all_history_ids = set()
    for ids in history_property_ids.values():
        all_history_ids.update(ids)
    
    # ç•¶å‰é€±æ¬¡çš„æ¡ˆä»¶ç·¨è™Ÿ
    current_ids = set(p['property_id'] for p in current_properties if p.get('property_id'))
    
    # è¨ˆç®—æ¯å€‹æˆ¿æºçš„ç‹€æ…‹
    property_dict = {p['property_id']: p for p in current_properties if p.get('property_id')}
    
    for prop_id, prop in property_dict.items():
        # æª¢æŸ¥é€™å€‹æ¡ˆä»¶åœ¨æ­·å²ä¸­å‡ºç¾éå¹¾æ¬¡
        weeks_seen = 0
        first_seen_week = current_week_id
        
        for week in reversed(history_weeks):  # å¾æœ€èˆŠçš„é–‹å§‹æª¢æŸ¥
            if prop_id in history_property_ids.get(week, set()):
                weeks_seen += 1
                first_seen_week = week
        
        if weeks_seen == 0:
            # æ–°å¢æ¡ˆä»¶ï¼ˆæœ¬é€±é¦–æ¬¡å‡ºç¾ï¼‰
            prop['status'] = 'new'
            prop['weeks_active'] = 1
            prop['first_seen_week'] = current_week_id
        else:
            # æŒçºŒæ¡ˆä»¶
            prop['status'] = 'active'
            prop['weeks_active'] = weeks_seen + 1  # åŠ ä¸Šç•¶å‰é€±
            prop['first_seen_week'] = first_seen_week
    
    # æª¢æŸ¥æ¶ˆå¤±çš„æ¡ˆä»¶ï¼ˆä¸Šé€±æœ‰ä½†æœ¬é€±æ²’æœ‰ï¼‰
    result_properties = list(property_dict.values())
    
    if history_weeks:
        last_week = history_weeks[0]  # ä¸Šä¸€é€±
        last_week_ids = history_property_ids.get(last_week, set())
        disappeared_ids = last_week_ids - current_ids
        
        # è¼‰å…¥ä¸Šé€±çš„å®Œæ•´è³‡æ–™ä»¥ç²å–æ¶ˆå¤±æ¡ˆä»¶çš„è©³ç´°è³‡è¨Š
        if disappeared_ids:
            last_week_properties = load_csv_data(city, district, building_type, property_category, last_week)
            for prop in last_week_properties:
                if prop.get('property_id') in disappeared_ids:
                    prop['status'] = 'inactive'
                    prop['weeks_active'] = 0
                    prop['disappeared_week'] = current_week_id
                    result_properties.append(prop)
    
    return result_properties

def process_dataframe(df: pd.DataFrame, city: str, district: str, building_type: str, property_category: str, week_id: str) -> List[dict]:
    """è™•ç† DataFrame ä¸¦è½‰æ›ç‚ºæˆ¿æºåˆ—è¡¨"""
    properties = []
    
    for _, row in df.iterrows():
        property_id = row.get('æ¡ˆä»¶ç·¨è™Ÿ', '')
        if pd.isna(property_id) or not property_id:
            continue
        property_id = str(int(property_id) if isinstance(property_id, float) else property_id)
        
        title = str(row.get('æ¨™é¡Œ', ''))
        
        raw_address = str(row.get('åœ°å€', ''))
        if city and not raw_address.startswith(city):
            raw_address = city + raw_address
        if district and district not in raw_address:
            raw_address = raw_address.replace(city, city + district)
        address = raw_address
        
        rent = row.get('ç§Ÿé‡‘', 0)
        if pd.isna(rent):
            rent = 0
        rent = int(rent)
        
        area = row.get('åªæ•¸', row.get('å¡æ•¸', 0))
        if pd.isna(area):
            area = 0
        area = float(area)
        
        room_type = str(row.get('æˆ¿å‹', ''))
        if room_type == 'nan':
            room_type = ''
        
        floor = str(row.get('æ¨“å±¤', ''))
        if floor == 'nan':
            floor = ''
        
        building_type_val = building_type or 'unknown'
        property_category_val = property_category or ''
        
        latitude = 0
        longitude = 0
        
        if 'ç·¯åº¦' in df.columns and 'ç¶“åº¦' in df.columns:
            lat_val = row.get('ç·¯åº¦', 0)
            lng_val = row.get('ç¶“åº¦', 0)
            if not pd.isna(lat_val) and not pd.isna(lng_val):
                latitude = float(lat_val)
                longitude = float(lng_val)
        
        if latitude == 0 and longitude == 0 and 'åº§æ¨™' in df.columns:
            coord_str = row.get('åº§æ¨™', '')
            if not pd.isna(coord_str):
                latitude, longitude = parse_dms_coordinate(str(coord_str))
        
        prop_week_id = row.get('é€±æ¬¡', row.get('å¹´é€±', ''))
        if pd.isna(prop_week_id) or not prop_week_id:
            prop_week_id = week_id or get_week_id()
        prop_week_id = str(prop_week_id)
        if prop_week_id.endswith('.0'):
            prop_week_id = prop_week_id[:-2]
        
        if not address or rent <= 0:
            continue
        
        properties.append({
            'property_id': property_id,
            'title': title,
            'address': address,
            'rent_monthly': rent,
            'area': area,
            'room_type': room_type,
            'floor': floor,
            'latitude': latitude,
            'longitude': longitude,
            'building_type': building_type_val,
            'property_category': property_category_val,
            'upload_week': prop_week_id,
            'status': 'active'
        })
    
    return properties

# ============ API ç«¯é» ============

@app.get("/api/versions")
async def get_versions():
    """ç²å–æ‰€æœ‰å¯ç”¨çš„é€±æ¬¡ç‰ˆæœ¬"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        conn.close()
        return {"status": "success", "versions": versions, "count": len(versions)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/available-filters")
async def get_available_filters():
    """ç²å–å¯ç”¨çš„ç¯©é¸é¸é …ï¼ˆåŸºæ–¼ç¾æœ‰ CSV æ–‡ä»¶ï¼‰"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT city, district FROM csv_index WHERE district != '' ORDER BY city, district")
        districts = [{"city": row[0], "district": row[1]} for row in cursor.fetchall()]
        
        cursor.execute("SELECT DISTINCT building_type FROM csv_index WHERE building_type != ''")
        building_types = [row[0] for row in cursor.fetchall()]
        
        cursor.execute("SELECT DISTINCT property_category FROM csv_index WHERE property_category != ''")
        property_categories = [row[0] for row in cursor.fetchall()]
        
        cursor.execute("SELECT DISTINCT week_id FROM csv_index WHERE week_id != '' ORDER BY week_id DESC")
        week_ids = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            "status": "success",
            "filters": {
                "districts": districts,
                "building_types": building_types,
                "property_categories": property_categories,
                "week_ids": week_ids
            },
            "google_drive_available": drive_available
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis_v4")
async def analysis_v4(
    address: str,
    city: Optional[str] = None,
    district: Optional[str] = None,
    distance_min: int = 0,
    distance_max: int = 5000,
    building_type: Optional[str] = None,
    property_category: Optional[str] = None,
    room_type: Optional[str] = None,
    week_id: Optional[str] = None,
    lat: Optional[float] = None,
    lng: Optional[float] = None
):
    """åˆ†æ API - æŒ‰éœ€è¼‰å…¥æŒ‡å®šæ¢ä»¶çš„æ•¸æ“š"""
    try:
        # ä¿®æ­£é–‹å§‹ï¼šè‡ªå‹•è™•ç† week_id é è¨­å€¼
        if not week_id:
            available_weeks = get_all_week_ids()
            if available_weeks:
                week_id = available_weeks[0]
                print(f"â„¹ï¸ å‰ç«¯æœªæŒ‡å®šé€±æ¬¡ï¼Œè‡ªå‹•é–å®šæœ€æ–°ç‰ˆæœ¬: {week_id}")
            else:
                week_id = get_week_id()
        # ä¿®æ­£çµæŸ

        if lat is not None and lng is not None and lat != 0 and lng != 0:
            query_lat, query_lon = lat, lng
        else:
            query_lat, query_lon = 25.0288, 121.4625
        
        if not district:
            districts = [
                'æ¿æ©‹å€', 'ä¸‰é‡å€', 'ä¸­å’Œå€', 'æ°¸å’Œå€', 'æ–°èŠå€', 'æ–°åº—å€', 'åœŸåŸå€',
                'è˜†æ´²å€', 'æ¨¹æ—å€', 'æ±æ­¢å€', 'é¶¯æ­Œå€', 'ä¸‰å³½å€', 'æ·¡æ°´å€',
                'äº”è‚¡å€', 'æ³°å±±å€', 'æ—å£å€', 'å…«é‡Œå€',
                'å¤§å®‰å€', 'ä¿¡ç¾©å€', 'ä¸­å±±å€', 'æ¾å±±å€', 'å—æ¸¯å€', 'å…§æ¹–å€'
            ]
            for d in districts:
                if d in address:
                    district = d
                    break
        
        load_category = None
        if room_type == 'å¥—æˆ¿':
            load_category = 'å¥—æˆ¿'
        elif room_type in ['2æˆ¿', '3æˆ¿', '3æˆ¿ä»¥ä¸Š']:
            load_category = 'ä½å®¶'
        elif property_category:
            load_category = property_category
        
        # æ ¹æ“šå€åŸŸè‡ªå‹•åˆ¤æ–·åŸå¸‚ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if not city:
            taipei_districts = ['ä¸­æ­£å€', 'å¤§åŒå€', 'ä¸­å±±å€', 'æ¾å±±å€', 'å¤§å®‰å€', 'è¬è¯å€', 'ä¿¡ç¾©å€', 'å£«æ—å€', 'åŒ—æŠ•å€', 'å…§æ¹–å€', 'å—æ¸¯å€', 'æ–‡å±±å€']
            if district in taipei_districts:
                city = 'å°åŒ—å¸‚'
            else:
                city = 'æ–°åŒ—å¸‚'
        
        all_properties = load_csv_data(
            city=city,
            district=district,
            building_type=building_type,
            property_category=load_category,
            week_id=week_id
        )
        
        # è¨ˆç®—æˆ¿æºç‹€æ…‹ï¼ˆæ–°å¢/æŒçºŒ/æ¶ˆå¤±ï¼‰
        if week_id:
            all_properties = calculate_property_status(
                all_properties, city, district, building_type, load_category, week_id
            )
        else:
            # æ²’æœ‰æŒ‡å®šé€±æ¬¡ï¼Œé è¨­ç‚ºæ–°å¢
            for prop in all_properties:
                prop['status'] = 'new'
                prop['weeks_active'] = 1
        
        # å»é™¤é‡è¤‡æ¡ˆä»¶ï¼ˆä¾æ“šæ¡ˆä»¶ç·¨è™Ÿï¼‰
        seen_ids = set()
        unique_properties = []
        for prop in all_properties:
            prop_id = prop.get('property_id')
            if prop_id and prop_id not in seen_ids:
                seen_ids.add(prop_id)
                unique_properties.append(prop)
        all_properties = unique_properties
        
        filtered_properties = []
        for prop in all_properties:
            if prop['latitude'] == 0 and prop['longitude'] == 0:
                continue
            
            distance = haversine_distance(query_lat, query_lon, prop['latitude'], prop['longitude'])
            
            if distance_min <= distance <= distance_max:
                prop['distance'] = distance
                
                if room_type and room_type != 'å…¨éƒ¨':
                    if room_type == 'å¥—æˆ¿':
                        if prop.get('property_category') != 'å¥—æˆ¿' and 'å¥—æˆ¿' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '2æˆ¿':
                        if '2' not in prop.get('room_type', '') and 'å…©' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '3æˆ¿':
                        if '3' not in prop.get('room_type', '') and 'ä¸‰' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '3æˆ¿ä»¥ä¸Š':
                        rt = prop.get('room_type', '')
                        has_large = any(str(n) in rt for n in range(4, 10)) or any(c in rt for c in ['å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹'])
                        if not has_large:
                            continue
                
                filtered_properties.append(prop)
        
        # åˆ†é¡çµ±è¨ˆ
        new_properties = [p for p in filtered_properties if p.get('status') == 'new']
        active_properties = [p for p in filtered_properties if p.get('status') == 'active']
        inactive_properties = [p for p in filtered_properties if p.get('status') == 'inactive']
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“šï¼ˆæ’é™¤æ¶ˆå¤±çš„æ¡ˆä»¶ï¼‰
        available_properties = new_properties + active_properties
        
        if available_properties:
            avg_rent = sum(p['rent_monthly'] for p in available_properties) / len(available_properties)
            min_rent = min(p['rent_monthly'] for p in available_properties)
            max_rent = max(p['rent_monthly'] for p in available_properties)
            avg_area = sum(p['area'] for p in available_properties if p['area'] > 0) / max(1, len([p for p in available_properties if p['area'] > 0]))
        else:
            avg_rent = min_rent = max_rent = avg_area = 0
        
        room_type_counts = {}
        for p in available_properties:
            rt = p['room_type'] or 'æœªçŸ¥'
            room_type_counts[rt] = room_type_counts.get(rt, 0) + 1
        
        room_type_analysis = [{"room_type": rt, "count": count} for rt, count in sorted(room_type_counts.items(), key=lambda x: -x[1])]
        
        return {
            "status": "success",
            "query": {
                "address": address,
                "district": district,
                "coordinates": {"latitude": query_lat, "longitude": query_lon},
                "distance_range": {"min": distance_min, "max": distance_max},
                "building_type": building_type,
                "property_category": load_category,
                "room_type": room_type,
                "week_id": week_id or "current"
            },
            "summary": {
                "total_properties": len(filtered_properties),
                "available_properties": len(available_properties),
                "new_properties": len(new_properties),
                "active_properties": len(active_properties),
                "inactive_properties": len(inactive_properties),
                "avg_rent_all": round(avg_rent),
                "min_rent": min_rent,
                "max_rent": max_rent,
                "avg_area": round(avg_area, 1)
            },
            "properties": filtered_properties,
            "room_type_analysis": room_type_analysis,
            "data_source": "google_drive" if drive_available else "local"
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

class ResetRequest(BaseModel):
    password: str

@app.post("/api/admin/reset-database")
async def reset_database(request: ResetRequest):
    """é‡ç½®æ•¸æ“šåº«ä¸¦é‡æ–°æƒæ CSV"""
    if request.password != "1234":
        raise HTTPException(status_code=403, detail="å¯†ç¢¼éŒ¯èª¤")
    
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM csv_index")
        cursor.execute("DELETE FROM versions")
        conn.commit()
        conn.close()
        scan_available_csv_files()
        return {"status": "success", "message": "æ•¸æ“šåº«å·²é‡ç½®ä¸¦é‡æ–°æƒæ CSV æ–‡ä»¶"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/admin/database-status")
async def database_status():
    """ç²å–æ•¸æ“šåº«ç‹€æ…‹"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM csv_index")
        csv_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT SUM(record_count) FROM csv_index")
        total_records = cursor.fetchone()[0] or 0
        
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        
        cursor.execute("SELECT filename, city, district, building_type, property_category, week_id, record_count, source, file_id FROM csv_index ORDER BY city, district, building_type, property_category")
        csv_files = [{"filename": row[0], "city": row[1], "district": row[2], "building_type": row[3], "property_category": row[4], "week_id": row[5], "record_count": row[6], "source": row[7], "file_id": row[8]} for row in cursor.fetchall()]
        
        conn.close()
        
        # åŠ å…¥å¿«å–ç‹€æ…‹
        cache_stats = get_cache_stats()
        
        return {
            "status": "success",
            "database": {
                "csv_files_count": csv_count,
                "total_records": total_records,
                "versions_count": len(versions),
                "versions": versions,
                "csv_files": csv_files
            },
            "google_drive": {
                "available": drive_available,
                "folder_id": drive_folder_id if drive_available else None
            },
            "cache": cache_stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/admin/rescan-csv")
async def rescan_csv():
    """é‡æ–°æƒæ CSV æ–‡ä»¶"""
    try:
        scan_available_csv_files()
        
        # è¿”å›æƒæçµæœçš„è©³ç´°è³‡è¨Š
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM csv_index")
        count = cursor.fetchone()[0]
        cursor.execute("SELECT DISTINCT city FROM csv_index")
        cities = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        return {
            "status": "success", 
            "message": "CSV æ–‡ä»¶å·²é‡æ–°æƒæ",
            "indexed_files": count,
            "cities": cities,
            "drive_available": drive_available,
            "drive_folder_id": drive_folder_id
        }
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "message": str(e),
            "traceback": traceback.format_exc()
        }

@app.get("/api/admin/drive-status")
async def get_drive_status():
    """è¨ºæ–· Google Drive é€£æ¥ç‹€æ…‹"""
    result = {
        "drive_available": drive_available,
        "drive_folder_id": drive_folder_id,
        "drive_folder_name": GOOGLE_DRIVE_FOLDER_NAME,
        "has_service": drive_service is not None,
        "env_key_exists": os.getenv('GOOGLE_DRIVE_KEY_JSON') is not None,
        "files_found": [],
        "error": None
    }
    
    if drive_available and drive_folder_id:
        try:
            files = list_google_drive_files(drive_folder_id)
            result["files_found"] = files[:50]  # åªè¿”å›å‰ 50 å€‹æª”æ¡ˆ
            result["total_files"] = len(files)
        except Exception as e:
            result["error"] = str(e)
    
    return result

@app.get("/api/admin/test-download")
async def test_download(city: str = "å°åŒ—å¸‚", district: str = "å¤§å®‰å€", week_id: str = "2604"):
    """æ¸¬è©¦å¾ Google Drive ä¸‹è¼‰ CSV æª”æ¡ˆï¼ˆä½¿ç”¨å¿«å–ï¼‰"""
    result = {
        "city": city,
        "district": district,
        "week_id": week_id,
        "city_variants": [],
        "query_result": [],
        "download_result": [],
        "cache_used": False,
        "error": None
    }
    
    try:
        # ç²å–åŸå¸‚åç¨±çš„æ‰€æœ‰è®Šé«”
        city_variants = normalize_city_name(city)
        result["city_variants"] = city_variants
        
        # å¾æ•¸æ“šåº«æŸ¥è©¢åŒ¹é…çš„æª”æ¡ˆï¼ˆæ”¯æ´åŸå¸‚åç¨±è®Šé«”ï¼‰
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        if city_variants:
            placeholders = ','.join(['?' for _ in city_variants])
            cursor.execute(f"""
                SELECT filename, file_id, city, district, building_type, property_category, week_id, source 
                FROM csv_index 
                WHERE city IN ({placeholders}) AND district = ? AND week_id = ? 
                AND source = 'google_drive' AND file_id IS NOT NULL
            """, city_variants + [district, week_id])
        else:
            cursor.execute("""
                SELECT filename, file_id, city, district, building_type, property_category, week_id, source 
                FROM csv_index 
                WHERE district = ? AND week_id = ? 
                AND source = 'google_drive' AND file_id IS NOT NULL
            """, [district, week_id])
        
        rows = cursor.fetchall()
        conn.close()
        
        result["query_result"] = [
            {"filename": r[0], "file_id": r[1], "city": r[2], "district": r[3], 
             "building_type": r[4], "property_category": r[5], "week_id": r[6], "source": r[7]}
            for r in rows
        ]
        
        # å˜—è©¦ä¸‹è¼‰ç¬¬ä¸€å€‹æª”æ¡ˆï¼ˆä½¿ç”¨å¿«å–ï¼‰
        if rows and drive_available and drive_service:
            filename, file_id = rows[0][0], rows[0][1]
            
            # æª¢æŸ¥å¿«å–
            cache_path = get_cache_path(file_id)
            if is_cache_valid(cache_path):
                result["cache_used"] = True
            
            try:
                df = download_file_from_drive(file_id, filename)
                
                if df is not None:
                    result["download_result"].append({
                        "filename": filename,
                        "file_id": file_id,
                        "success": True,
                        "rows": len(df),
                        "columns": list(df.columns),
                        "sample": df.head(2).to_dict('records'),
                        "from_cache": result["cache_used"]
                    })
                else:
                    result["download_result"].append({
                        "filename": filename,
                        "file_id": file_id,
                        "success": False,
                        "error": "DataFrame is None"
                    })
            except Exception as e:
                result["download_result"].append({
                    "filename": filename,
                    "file_id": file_id,
                    "success": False,
                    "error": str(e)
                })
    except Exception as e:
        result["error"] = str(e)
        import traceback
        result["traceback"] = traceback.format_exc()
    
    return result

@app.get("/api/admin/cache-status")
async def cache_status():
    """ç²å–å¿«å–ç‹€æ…‹"""
    return {
        "status": "success",
        "cache_dir": CACHE_DIR,
        "cache_expiry_hours": CACHE_EXPIRY_HOURS,
        "stats": get_cache_stats()
    }

@app.post("/api/admin/clear-cache")
async def clear_cache_api():
    """æ¸…é™¤æ‰€æœ‰å¿«å–"""
    try:
        success = clear_cache()
        return {
            "status": "success" if success else "no_cache",
            "message": "å¿«å–å·²æ¸…é™¤" if success else "æ²’æœ‰å¿«å–éœ€è¦æ¸…é™¤"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# éœæ…‹æ–‡ä»¶æœå‹™
static_dir = os.path.dirname(__file__)
if os.path.exists(static_dir):
    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")