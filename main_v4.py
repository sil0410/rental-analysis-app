"""
ç§Ÿå±‹è¡Œæƒ…åˆ†æç³»çµ± - ç‰ˆæœ¬æ§åˆ¶ API v7.5 (Database Architecture)
æ¶æ§‹è®Šæ›´ï¼šETL æ¨¡å¼
1. Source: Google Drive (CSV)
2. Storage: SQLite (Properties Table)
3. Query: Direct SQL Select
"""

import sqlite3
import json
import os
import re
from datetime import datetime
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import math
import pandas as pd
from io import BytesIO

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="ç§Ÿå±‹è¡Œæƒ…åˆ†æ API v7.5 (DBç‰ˆ)")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•¸æ“šåº«è·¯å¾‘
DB_PATH = os.path.join(os.path.dirname(__file__), "rental.db")
ADMIN_PASSWORD = os.getenv("ADMIN_PASSWORD", "1234")  # å»ºè­°å¾ç’°å¢ƒè®Šæ•¸è®€å–

# ============ Google Drive é…ç½® ============
GOOGLE_DRIVE_FOLDER_NAME = "ç§Ÿå±‹æ•¸æ“š"
drive_service = None
drive_folder_id = None
drive_available = False

def init_google_drive():
    """åˆå§‹åŒ– Google Drive API"""
    global drive_service, drive_folder_id, drive_available
    
    try:
        key_json_str = os.getenv('GOOGLE_DRIVE_KEY_JSON')
        if not key_json_str:
            print("â„¹ï¸ Google Drive æœªé…ç½® (ä½¿ç”¨æœ¬åœ°æ¨¡å¼)")
            return False
        
        try:
            from google.oauth2.service_account import Credentials
            from googleapiclient.discovery import build
        except ImportError:
            print("âš ï¸ ç¼ºå°‘ Google Drive å¥—ä»¶: pip install google-auth google-api-python-client")
            return False
        
        try:
            key_dict = json.loads(key_json_str)
        except json.JSONDecodeError:
            print("âš ï¸ Google Drive Key JSON è§£æå¤±æ•—")
            return False
        
        credentials = Credentials.from_service_account_info(
            key_dict, scopes=['https://www.googleapis.com/auth/drive']
        )
        
        drive_service = build('drive', 'v3', credentials=credentials)
        
        results = drive_service.files().list(
            q=f"name='{GOOGLE_DRIVE_FOLDER_NAME}' and mimeType='application/vnd.google-apps.folder' and trashed=false",
            spaces='drive', fields='files(id, name)', pageSize=1
        ).execute()
        
        files = results.get('files', [])
        if files:
            drive_folder_id = files[0]['id']
            drive_available = True
            print(f"âœ“ Google Drive é€£æ¥æˆåŠŸ (ID: {drive_folder_id})")
            return True
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {GOOGLE_DRIVE_FOLDER_NAME}")
            return False
            
    except Exception as e:
        print(f"âš ï¸ Google Drive åˆå§‹åŒ–ç•°å¸¸: {e}")
        return False

# ============ æ•¸æ“šåº«åˆå§‹åŒ– (Schema) ============

def init_database():
    """åˆå§‹åŒ–æ•¸æ“šåº«æ¶æ§‹ - æ–°å¢ properties è¡¨"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # 1. ç‰ˆæœ¬æ§åˆ¶è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS versions (
            week_id TEXT PRIMARY KEY,
            upload_date TEXT NOT NULL,
            record_count INTEGER DEFAULT 0
        )
    """)
    
    # 2. æª”æ¡ˆåŒæ­¥è¨˜éŒ„è¡¨ (å–ä»£èˆŠçš„ csv_index)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS sync_log (
            file_id TEXT PRIMARY KEY,
            filename TEXT,
            city TEXT,
            district TEXT,
            week_id TEXT,
            status TEXT, -- 'synced', 'failed'
            synced_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # 3. æ ¸å¿ƒè³‡æ–™è¡¨ (å­˜æ”¾æ‰€æœ‰æˆ¿æºæ•¸æ“š)
    # é€™å°±æ˜¯ä½ çš„ã€Œå¤§å†°ç®±ã€ï¼Œæ‰€æœ‰ CSV çš„è³‡æ–™éƒ½æœƒè¢«æ¸…æ´—å¾Œæ”¾å…¥é€™è£¡
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS properties (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            property_id TEXT,          -- æ¡ˆä»¶ç·¨è™Ÿ
            title TEXT,
            address TEXT,
            city TEXT,
            district TEXT,
            rent INTEGER,
            area REAL,
            floor TEXT,
            room_type TEXT,            -- åŸå§‹æˆ¿å‹ (ä¾‹å¦‚: 2æˆ¿1å»³)
            property_category TEXT,    -- æ­¸é¡ (å¥—æˆ¿/ä½å®¶)
            building_type TEXT,        -- å»ºç‰©é¡å‹ (apartment/building)
            latitude REAL,
            longitude REAL,
            week_id TEXT,              -- æ™‚é–“ç¶­åº¦
            file_id TEXT,              -- ä¾†æºæª”æ¡ˆ
            
            -- è¤‡åˆå”¯ä¸€éµï¼šç¢ºä¿åŒä¸€é€±ã€åŒä¸€å€‹æ¡ˆä»¶ç·¨è™Ÿåªæœƒå­˜ä¸€æ¬¡
            UNIQUE(property_id, week_id)
        )
    """)
    
    # å»ºç«‹ç´¢å¼•ä»¥åŠ é€ŸæŸ¥è©¢
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_loc ON properties (city, district)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_rent ON properties (rent)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_week ON properties (week_id)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_btype ON properties (building_type)")
    
    conn.commit()
    conn.close()
    print("âœ“ æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ (Tables: versions, sync_log, properties)")

# ============ å·¥å…·å‡½æ•¸ ============

def parse_dms_coordinate(coord_str: str):
    """è§£æåº§æ¨™ (æ”¯æ´åº¦åˆ†ç§’èˆ‡åé€²ä½)"""
    if not coord_str or pd.isna(coord_str):
        return 0, 0
    
    coord_str = str(coord_str).strip()
    
    # 1. å˜—è©¦è§£æåº¦åˆ†ç§’ (DMS)
    dms_pattern = r"(\d+)Â°(\d+)'(\d+(?:\.\d+)?)\"([NSEW])"
    matches = re.findall(dms_pattern, coord_str)
    if len(matches) >= 2:
        try:
            lat_match = next((m for m in matches if m[3] in ['N', 'S']), None)
            lng_match = next((m for m in matches if m[3] in ['E', 'W']), None)
            
            if lat_match and lng_match:
                lat = float(lat_match[0]) + float(lat_match[1])/60 + float(lat_match[2])/3600
                if lat_match[3] == 'S': lat = -lat
                
                lng = float(lng_match[0]) + float(lng_match[1])/60 + float(lng_match[2])/3600
                if lng_match[3] == 'W': lng = -lng
                return lat, lng
        except:
            pass

    # 2. å˜—è©¦è§£æç›´æ¥çš„æµ®é»æ•¸ (Decimal)
    try:
        # æœ‰äº›è³‡æ–™å¯èƒ½æ˜¯ "25.123, 121.456" æˆ–å–®ç´”æµ®é»æ•¸æ¬„ä½
        parts = re.findall(r"[-+]?\d*\.\d+|\d+", coord_str)
        if len(parts) >= 2:
            # å°ç£å¤§ç´„åœ¨ Lat 22-25, Lng 120-122ï¼Œç°¡å–®åˆ¤æ–·
            v1, v2 = float(parts[0]), float(parts[1])
            if 20 <= v1 <= 26 and 118 <= v2 <= 124:
                return v1, v2
            elif 20 <= v2 <= 26 and 118 <= v1 <= 124:
                return v2, v1
    except:
        pass
        
    return 0, 0

def haversine_distance(lat1, lon1, lat2, lon2):
    """è¨ˆç®—å…©é»è·é›¢ (å…¬å°º)"""
    R = 6371000
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi/2)**2 + math.cos(phi1)*math.cos(phi2)*math.sin(dlambda/2)**2
    return R * 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

def parse_filename_info(filename: str):
    """å¾æª”åè§£æ metadata"""
    info = {
        'week_id': '', 'city': '', 'district': '', 
        'building_type': 'unknown', 'property_category': 'unknown'
    }
    
    # è§£æé€±æ¬¡
    week_match = re.search(r'_(\d{4})(?:_merged)?(?:\.csv)?$', filename)
    if week_match:
        info['week_id'] = week_match.group(1)
    
    # è§£æé¡å‹
    if 'é›»æ¢¯' in filename: info['building_type'] = 'building'
    elif 'å…¬å¯“' in filename: info['building_type'] = 'apartment'
    
    if 'å¥—æˆ¿' in filename: info['property_category'] = 'å¥—æˆ¿'
    elif 'ä½å®¶' in filename: info['property_category'] = 'ä½å®¶'
    
    # è§£æåœ°é»
    if filename.startswith('æ–°åŒ—å¸‚'): info['city'] = 'æ–°åŒ—å¸‚'
    elif filename.startswith('è‡ºåŒ—å¸‚') or filename.startswith('å°åŒ—å¸‚'): info['city'] = 'å°åŒ—å¸‚'
    elif filename.startswith('åŸºéš†å¸‚'): info['city'] = 'åŸºéš†å¸‚'
    elif filename.startswith('æ¡ƒåœ’å¸‚'): info['city'] = 'æ¡ƒåœ’å¸‚'
    
    # ç°¡æ˜“å€åŸŸåˆ¤æ–·
    districts = ['æ¿æ©‹', 'ä¸‰é‡', 'ä¸­å’Œ', 'æ°¸å’Œ', 'æ–°èŠ', 'æ–°åº—', 'åœŸåŸ', 'è˜†æ´²', 'æ¨¹æ—', 'æ±æ­¢', 'æ—å£', 'æ·¡æ°´', 'å¤§å®‰', 'ä¿¡ç¾©', 'ä¸­å±±', 'æ¾å±±', 'å…§æ¹–']
    for d in districts:
        if d in filename:
            info['district'] = d + 'å€' if not d.endswith('å€') else d
            break
            
    return info

# ============ ETL æ ¸å¿ƒé‚è¼¯ (Sync Data) ============

async def process_sync_task(background_tasks: BackgroundTasks):
    """èƒŒæ™¯åŸ·è¡Œï¼šåŒæ­¥ Drive è³‡æ–™åˆ° DB"""
    if not drive_available or not drive_service:
        print("âš ï¸ ç„¡æ³•åŒæ­¥ï¼šGoogle Drive æœªé€£æ¥")
        return

    print("ğŸ”„ é–‹å§‹åŸ·è¡Œè³‡æ–™åŒæ­¥ä»»å‹™...")
    
    # 1. ç²å– Drive ä¸Šçš„æ‰€æœ‰ CSV
    try:
        results = drive_service.files().list(
            q=f"'{drive_folder_id}' in parents and name contains '.csv' and trashed=false",
            fields="files(id, name)", pageSize=1000
        ).execute()
        drive_files = results.get('files', [])
    except Exception as e:
        print(f"âš ï¸ è®€å– Drive åˆ—è¡¨å¤±æ•—: {e}")
        return

    # 2. æª¢æŸ¥å“ªäº›å·²ç¶“åŒæ­¥é
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT file_id FROM sync_log")
    synced_ids = {row[0] for row in cursor.fetchall()}
    
    new_files = [f for f in drive_files if f['id'] not in synced_ids]
    print(f"ğŸ“Š æƒæçµæœï¼šå…± {len(drive_files)} å€‹æª”æ¡ˆï¼Œéœ€åŒæ­¥ {len(new_files)} å€‹æ–°æª”æ¡ˆ")
    
    # 3. é€ä¸€ä¸‹è¼‰ä¸¦åŒ¯å…¥
    from googleapiclient.http import MediaIoBaseDownload
    
    count_success = 0
    for file_meta in new_files:
        file_id = file_meta['id']
        filename = file_meta['name']
        print(f"  â¬‡ï¸ ä¸‹è¼‰ä¸¦è™•ç†: {filename} ...")
        
        try:
            # è§£ææª”åè³‡è¨Š
            meta = parse_filename_info(filename)
            if not meta['week_id']:
                # å¦‚æœæª”åæ²’æœ‰é€±æ¬¡ï¼Œè·³éæˆ–ä½¿ç”¨ç•¶å‰é€±æ¬¡ (é€™è£¡é¸æ“‡è·³éä»¥ä¿è­‰æ•¸æ“šå“è³ª)
                print(f"     âš ï¸ è·³é (ç„¡æ³•è§£æé€±æ¬¡): {filename}")
                continue

            # ä¸‹è¼‰å…§å®¹
            request = drive_service.files().get_media(fileId=file_id)
            fh = BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done:
                _, done = downloader.next_chunk()
            fh.seek(0)
            
            # Pandas è®€å–èˆ‡æ¸…ç†
            df = pd.read_csv(fh, encoding='utf-8-sig')
            
            # æ•¸æ“šè½‰æ› (Transform)
            clean_rows = []
            for _, row in df.iterrows():
                try:
                    # å¿…å¡«æ¬„ä½æª¢æŸ¥
                    pid = str(row.get('æ¡ˆä»¶ç·¨è™Ÿ', ''))
                    if not pid or pid == 'nan': continue
                    
                    rent = row.get('ç§Ÿé‡‘', 0)
                    if pd.isna(rent): rent = 0
                    else: rent = int(str(rent).replace(',', '').split('.')[0])
                    
                    # åº§æ¨™è™•ç†
                    lat, lng = 0.0, 0.0
                    if 'ç·¯åº¦' in df.columns and 'ç¶“åº¦' in df.columns and not pd.isna(row['ç·¯åº¦']):
                        lat, lng = float(row['ç·¯åº¦']), float(row['ç¶“åº¦'])
                    elif 'åº§æ¨™' in df.columns:
                        lat, lng = parse_dms_coordinate(row.get('åº§æ¨™', ''))
                    
                    # åœ°å€è£œå…¨
                    addr = str(row.get('åœ°å€', ''))
                    if meta['city'] and not addr.startswith(meta['city']):
                        addr = meta['city'] + addr
                    
                    clean_rows.append((
                        pid,
                        str(row.get('æ¨™é¡Œ', '')),
                        addr,
                        meta['city'],
                        meta['district'] or row.get('å€åŸŸ', ''), # å¦‚æœæª”åæ²’å€åŸŸï¼Œçœ‹CSVå…§æœ‰ç„¡
                        rent,
                        float(row.get('åªæ•¸', 0) or 0),
                        str(row.get('æ¨“å±¤', '')),
                        str(row.get('æˆ¿å‹', '')),
                        meta['property_category'],
                        meta['building_type'],
                        lat,
                        lng,
                        meta['week_id'],
                        file_id
                    ))
                except Exception as e:
                    continue # å–®è¡Œå¤±æ•—ä¸å½±éŸ¿æ•´æª”
            
            # æ‰¹é‡å¯«å…¥ (Load)
            if clean_rows:
                cursor.executemany("""
                    INSERT OR IGNORE INTO properties 
                    (property_id, title, address, city, district, rent, area, floor, room_type, 
                     property_category, building_type, latitude, longitude, week_id, file_id)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, clean_rows)
                
                # æ›´æ–°ç‰ˆæœ¬è¡¨
                cursor.execute("""
                    INSERT OR IGNORE INTO versions (week_id, upload_date) VALUES (?, ?)
                """, (meta['week_id'], datetime.now().strftime("%Y-%m-%d")))
            
            # è¨˜éŒ„åŒæ­¥æˆåŠŸ
            cursor.execute("""
                INSERT INTO sync_log (file_id, filename, city, district, week_id, status)
                VALUES (?, ?, ?, ?, ?, 'synced')
            """, (file_id, filename, meta['city'], meta['district'], meta['week_id']))
            
            conn.commit()
            count_success += 1
            print(f"     âœ“ æˆåŠŸåŒ¯å…¥ {len(clean_rows)} ç­†è³‡æ–™")
            
        except Exception as e:
            print(f"     âŒ è™•ç†å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    
    conn.close()
    print(f"ğŸ åŒæ­¥å®Œæˆï¼šæˆåŠŸè™•ç† {count_success} å€‹æª”æ¡ˆ")

# ============ API Endpoints ============

@app.on_event("startup")
async def startup_event():
    init_database()
    init_google_drive()

@app.get("/api/versions")
async def get_versions():
    """ç²å–å¯ç”¨çš„é€±æ¬¡"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
    versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
    conn.close()
    return {"status": "success", "versions": versions}

@app.get("/api/available-filters")
async def get_filters():
    """å¾ properties è¡¨å¿«é€Ÿç²å–ç¯©é¸æ¢ä»¶"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("SELECT DISTINCT city, district FROM properties WHERE district IS NOT NULL ORDER BY city, district")
    districts = [{"city": r[0], "district": r[1]} for r in cursor.fetchall()]
    
    cursor.execute("SELECT DISTINCT week_id FROM versions ORDER BY week_id DESC")
    week_ids = [r[0] for r in cursor.fetchall()]
    
    conn.close()
    return {
        "status": "success",
        "filters": {
            "districts": districts,
            "week_ids": week_ids,
            "building_types": ["apartment", "building"],
            "property_categories": ["å¥—æˆ¿", "ä½å®¶"]
        },
        "drive_connected": drive_available
    }

class AnalysisRequest(BaseModel):
    pass # GET è«‹æ±‚ä¸éœ€è¦ body definition, ä½†ç‚ºäº†çµæ§‹åŒ–å…ˆä¿ç•™

@app.get("/api/analysis_v4")
async def analysis_v4(
    address: str,
    city: Optional[str] = None,
    district: Optional[str] = None,
    distance_min: int = 0,
    distance_max: int = 5000,
    building_type: Optional[str] = None,
    property_category: Optional[str] = None,
    room_type: Optional[str] = None,
    week_id: Optional[str] = None,
    lat: Optional[float] = None,
    lng: Optional[float] = None
):
    """
    æ¥µé€ŸæŸ¥è©¢ API
    ç›´æ¥ä½¿ç”¨ SQL ç¯©é¸ï¼Œä¸å†è®€å– CSV
    """
    # 1. è™•ç†åƒæ•¸
    if lat is None or lng is None:
        # é è¨­åº§æ¨™ (æ–°åŒ—å¸‚æ”¿åºœ)
        q_lat, q_lng = 25.0117, 121.4651
    else:
        q_lat, q_lng = lat, lng

    # å¦‚æœæ²’æœ‰æŒ‡å®šé€±æ¬¡ï¼ŒæŠ“æœ€æ–°çš„
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row # è®“çµæœå¯ä»¥åƒå­—å…¸ä¸€æ¨£å­˜å–
    cursor = conn.cursor()

    if not week_id:
        cursor.execute("SELECT MAX(week_id) FROM versions")
        week_id = cursor.fetchone()[0]
        if not week_id:
            return {"status": "error", "message": "è³‡æ–™åº«ç‚ºç©ºï¼Œè«‹å…ˆåŸ·è¡ŒåŒæ­¥"}

    # 2. å»ºæ§‹ SQL æŸ¥è©¢
    # ç­–ç•¥ï¼šå…ˆç”¨ SQL ç¯©é¸å‡ºè©²å€åŸŸçš„æ‰€æœ‰æˆ¿æºï¼Œå†ç”¨ Python ç®—ç²¾ç¢ºè·é›¢ (å› ç‚º SQLite ç®—è·é›¢æ¯”è¼ƒéº»ç…©)
    # é€™æ¯”è¼‰å…¥æ•´å€‹ CSV å¿«å¾—å¤šï¼Œå› ç‚ºæˆ‘å€‘å·²ç¶“é™ç¸®åœ¨ç‰¹å®š district å’Œ week
    
    sql = """
        SELECT * FROM properties 
        WHERE week_id = ? 
    """
    params = [week_id]

    if district:
        sql += " AND district = ?"
        params.append(district)
    
    if building_type and building_type != 'å…¨éƒ¨':
        bt_val = 'apartment' if building_type == 'å…¬å¯“' else 'building' if building_type == 'é›»æ¢¯å¤§æ¨“' else building_type
        sql += " AND building_type = ?"
        params.append(bt_val)

    if property_category and property_category != 'å…¨éƒ¨':
        sql += " AND property_category = ?"
        params.append(property_category)

    # åŸ·è¡ŒæŸ¥è©¢
    cursor.execute(sql, params)
    rows = cursor.fetchall()
    
    # 3. è·é›¢éæ¿¾èˆ‡çµ±è¨ˆ
    filtered_props = []
    
    for row in rows:
        p_lat, p_lng = row['latitude'], row['longitude']
        
        # å¿½ç•¥æ²’æœ‰åº§æ¨™çš„è³‡æ–™
        if not p_lat or not p_lng:
            continue
            
        dist = haversine_distance(q_lat, q_lng, p_lat, p_lng)
        
        if distance_min <= dist <= distance_max:
            # æˆ¿å‹ç¯©é¸ (å› ç‚ºæˆ¿å‹æ–‡å­—å¾ˆé›œï¼Œç”¨ Python ç¯©é¸æ¯”è¼ƒå½ˆæ€§)
            r_type = row['room_type'] or ''
            if room_type:
                if room_type == 'å¥—æˆ¿' and ('å¥—' not in r_type and row['property_category'] != 'å¥—æˆ¿'): continue
                if room_type == '2æˆ¿' and '2' not in r_type and 'å…©' not in r_type: continue
                if room_type == '3æˆ¿' and '3' not in r_type and 'ä¸‰' not in r_type: continue
            
            # è½‰æ›ç‚ºå‰ç«¯éœ€è¦çš„æ ¼å¼
            prop_dict = dict(row)
            prop_dict['distance'] = dist
            prop_dict['rent_monthly'] = row['rent'] # å…¼å®¹èˆŠå‰ç«¯æ¬„ä½å
            filtered_props.append(prop_dict)

    conn.close()

    # 4. çµ±è¨ˆæ•¸æ“š
    if filtered_props:
        rents = [p['rent'] for p in filtered_props]
        avg_rent = sum(rents) / len(rents)
        min_rent = min(rents)
        max_rent = max(rents)
        areas = [p['area'] for p in filtered_props if p['area'] > 0]
        avg_area = sum(areas) / len(areas) if areas else 0
    else:
        avg_rent = min_rent = max_rent = avg_area = 0

    return {
        "status": "success",
        "query": {
            "district": district,
            "week_id": week_id,
            "count": len(filtered_props)
        },
        "summary": {
            "avg_rent_all": round(avg_rent),
            "min_rent": min_rent,
            "max_rent": max_rent,
            "avg_area": round(avg_area, 1),
            "total_properties": len(filtered_props)
        },
        "properties": filtered_props,
        "source": "database (ETL)"
    }

class AdminAction(BaseModel):
    password: str

@app.post("/api/admin/sync-data")
async def trigger_sync(action: AdminAction, background_tasks: BackgroundTasks):
    """
    è§¸ç™¼æ•¸æ“šåŒæ­¥ä»»å‹™ (éåŒæ­¥èƒŒæ™¯åŸ·è¡Œ)
    å°‡ Drive è³‡æ–™æ¬é‹åˆ° SQLite
    """
    if action.password != ADMIN_PASSWORD:
        raise HTTPException(status_code=403, detail="å¯†ç¢¼éŒ¯èª¤")
    
    if not drive_available:
        raise HTTPException(status_code=400, detail="Google Drive æœªé€£æ¥")

    # å•Ÿå‹•èƒŒæ™¯ä»»å‹™ï¼Œç«‹å³å›å‚³å›æ‡‰ï¼Œé¿å…å‰ç«¯è¶…æ™‚
    background_tasks.add_task(process_sync_task, background_tasks)
    
    return {"status": "success", "message": "åŒæ­¥ä»»å‹™å·²åœ¨èƒŒæ™¯å•Ÿå‹•ï¼Œè«‹ç¨å¾ŒæŸ¥çœ‹è³‡æ–™åº«ç‹€æ…‹"}

@app.post("/api/admin/reset-all")
async def reset_all(action: AdminAction):
    """
    å±éšªï¼šæ¸…ç©ºæ‰€æœ‰è³‡æ–™åº«å…§å®¹
    """
    if action.password != ADMIN_PASSWORD:
        raise HTTPException(status_code=403, detail="å¯†ç¢¼éŒ¯èª¤")
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("DELETE FROM properties")
    cursor.execute("DELETE FROM sync_log")
    cursor.execute("DELETE FROM versions")
    conn.commit()
    conn.close()
    
    return {"status": "success", "message": "è³‡æ–™åº«å·²å®Œå…¨æ¸…ç©º"}

@app.get("/api/admin/status")
async def admin_status():
    """æŸ¥çœ‹åŒæ­¥ç‹€æ…‹"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("SELECT COUNT(*) FROM properties")
    total_props = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM sync_log WHERE status='synced'")
    synced_files = cursor.fetchone()[0]
    
    cursor.execute("SELECT filename, status, synced_at FROM sync_log ORDER BY synced_at DESC LIMIT 5")
    recent_logs = [{"file": r[0], "status": r[1], "time": r[2]} for r in cursor.fetchall()]
    
    conn.close()
    
    return {
        "drive_connected": drive_available,
        "database": {
            "total_properties": total_props,
            "synced_files_count": synced_files,
            "recent_activity": recent_logs
        }
    }

# éœæ…‹æ–‡ä»¶
static_dir = os.path.dirname(__file__)
if os.path.exists(static_dir):
    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")