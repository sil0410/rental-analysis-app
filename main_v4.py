"""
ç§Ÿå±‹è¡Œæƒ…åˆ†æç³»çµ± - ç‰ˆæœ¬æ§åˆ¶ API v4.0
æ”¯æŒé€±æ¬¡ç®¡ç†ã€å‹•ç•«æ’­æ”¾ã€ç•™ç½®æ™‚é–“è‘—è‰²ã€å»ºç¯‰é¡å‹ç¯©é¸å’Œé€²éšæ¨¡å¼
"""

import sqlite3
import json
import os
import re
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional
import math

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="ç§Ÿå±‹è¡Œæƒ…åˆ†æ API v4.0")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============ æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶ ============

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚åˆå§‹åŒ–æ•¸æ“šåº«ä¸¦è‡ªå‹•å°å…¥ CSV"""
    init_database()
    auto_import_csv_files()


# æ•¸æ“šåº«è·¯å¾‘
DB_PATH = os.path.join(os.path.dirname(__file__), "rental.db")

# ============ æ•¸æ“šåº«åˆå§‹åŒ– ============

def init_database():
    """åˆå§‹åŒ–æ•¸æ“šåº«ï¼Œæ·»åŠ ç‰ˆæœ¬æ§åˆ¶å­—æ®µå’Œå»ºç¯‰é¡å‹"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç‰ˆæœ¬è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS versions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            week_id TEXT UNIQUE NOT NULL,
            upload_date TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # é¦–å…ˆå‰µå»º properties è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS properties (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            address TEXT,
            rent_monthly INTEGER,
            area REAL,
            room_type TEXT,
            floor TEXT,
            latitude REAL,
            longitude REAL,
            renovation_status TEXT,
            first_published_date TEXT,
            upload_week TEXT,
            status TEXT DEFAULT 'active',
            building_type TEXT DEFAULT 'apartment',
            deleted_date TEXT
        )
    """)
    
    # æª¢æŸ¥ properties è¡¨æ˜¯å¦æœ‰å¿…è¦å­—æ®µ
    cursor.execute("PRAGMA table_info(properties)")
    columns = {row[1] for row in cursor.fetchall()}
    
    if 'first_published_date' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN first_published_date TEXT")
    
    if 'deleted_date' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN deleted_date TEXT")
    
    if 'upload_week' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN upload_week TEXT")
    
    if 'status' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN status TEXT DEFAULT 'active'")
    
    if 'building_type' not in columns:
        cursor.execute("ALTER TABLE properties ADD COLUMN building_type TEXT DEFAULT 'apartment'")
    
    conn.commit()
    conn.close()

# ============ å·¥å…·å‡½æ•¸ ============

def get_week_id(date: datetime = None) -> str:
    """
    ç”Ÿæˆé€±æ¬¡ IDï¼Œæ ¼å¼ç‚º YYWW
    ä¾‹å¦‚ï¼š2601 è¡¨ç¤º 2026 å¹´ç¬¬ 01 é€±
    """
    if date is None:
        date = datetime.now()
    
    year = date.year % 100  # å–å¾Œå…©ä½å¹´ä»½
    week = date.isocalendar()[1]  # å– ISO é€±æ•¸
    
    return f"{year:02d}{week:02d}"

def extract_building_type_from_filename(filename: str) -> str:
    """å¾ CSV æ–‡ä»¶åæå–å»ºç¯‰é¡å‹"""
    if 'é›»æ¢¯å¤§æ¨“' in filename:
        return 'building'
    elif 'å…¬å¯“' in filename:
        return 'apartment'
    elif 'å¥—æˆ¿' in filename:
        return 'apartment'
    elif 'é€å¤©' in filename:
        return 'house'
    else:
        return 'apartment'  # é»˜èªå€¼

def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """è¨ˆç®—å…©é»ä¹‹é–“çš„è·é›¢ï¼ˆå…¬é‡Œï¼‰"""
    R = 6371  # åœ°çƒåŠå¾‘ï¼ˆå…¬é‡Œï¼‰
    
    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    delta_lat = math.radians(lat2 - lat1)
    delta_lon = math.radians(lon2 - lon1)
    
    a = math.sin(delta_lat / 2) ** 2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2) ** 2
    c = 2 * math.asin(math.sqrt(a))
    
    return R * c * 1000  # è½‰æ›ç‚ºå…¬å°º

def calculate_weeks_since_published(first_published_date: str) -> int:
    """è¨ˆç®—å¾é¦–æ¬¡ç™¼å¸ƒåˆ°ç¾åœ¨çš„é€±æ•¸"""
    if not first_published_date:
        return 0
    
    try:
        pub_date = datetime.strptime(first_published_date, "%Y-%m-%d")
        today = datetime.now()
        delta = today - pub_date
        return delta.days // 7
    except:
        return 0

# ============ ç‰ˆæœ¬ç®¡ç† API ============

@app.get("/api/versions")
async def get_versions():
    """ç²å–æ‰€æœ‰ç‰ˆæœ¬åˆ—è¡¨"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT week_id, upload_date 
            FROM versions 
            ORDER BY week_id ASC
        """)
        
        versions = [
            {'week_id': row[0], 'upload_date': row[1]}
            for row in cursor.fetchall()
        ]
        
        conn.close()
        
        return {
            'status': 'success',
            'versions': versions
        }
    
    except Exception as e:
        return {
            'status': 'error',
            'message': str(e)
        }

# ============ æ•¸æ“šå°å…¥ API ============


# ============ è‡ªå‹•å°å…¥ CSV ============

def auto_import_csv_files():
    """è‡ªå‹•å°å…¥ upload è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ CSV æª”æ¡ˆï¼ˆåˆä½µå°å…¥æ¨¡å¼ï¼‰"""
    import pandas as pd
    
    # ä½¿ç”¨å¤šå€‹å¯èƒ½çš„è·¯å¾‘
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "upload"),
        "/app/upload",
        "./upload",
        os.path.join(os.getcwd(), "upload")
    ]
    
    upload_dir = None
    for path in possible_paths:
        if os.path.exists(path):
            upload_dir = path
            print(f"âœ… æ‰¾åˆ° upload è³‡æ–™å¤¾: {upload_dir}")
            break
    
    # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œå˜—è©¦å‰µå»º
    if upload_dir is None:
        upload_dir = possible_paths[0]
        if not os.path.exists(upload_dir):
            os.makedirs(upload_dir)
            print(f"âœ… å·²å‰µå»º upload è³‡æ–™å¤¾: {upload_dir}")
            return
    
    # æƒææ‰€æœ‰ CSV æª”æ¡ˆ
    csv_files = [f for f in os.listdir(upload_dir) if f.endswith('.csv') and not f.endswith('_converted.csv')]
    
    if not csv_files:
        print("âš ï¸  upload è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆï¼Œé–‹å§‹åˆä½µå°å…¥...")
    print(f"ğŸ“‚ upload è³‡æ–™å¤¾è·¯å¾‘: {upload_dir}")
    
    # ç¬¬ä¸€æ­¥ï¼šåˆä½µæ‰€æœ‰ CSV æª”æ¡ˆ
    all_data = []
    for csv_filename in csv_files:
        try:
            csv_path = os.path.join(upload_dir, csv_filename)
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            all_data.append(df)
            print(f"  âœ“ è®€å–: {csv_filename} ({len(df)} è¡Œ)")
        except Exception as e:
            print(f"  âš ï¸  {csv_filename} è®€å–å¤±æ•—: {e}")
            continue
    
    if not all_data:
        print("âŒ æ²’æœ‰æˆåŠŸè®€å–ä»»ä½• CSV æª”æ¡ˆ")
        return
    
    # åˆä½µæ‰€æœ‰æ•¸æ“š
    merged_df = pd.concat(all_data, ignore_index=True)
    print(f"âœ… å·²åˆä½µ {len(merged_df)} è¡Œæ•¸æ“š")
    
    # å»é‡ï¼šä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç¾çš„æˆ¿æºï¼ˆåŸºæ–¼åœ°å€ + ç§Ÿé‡‘ï¼‰
    merged_df = merged_df.drop_duplicates(subset=['åœ°å€', 'ç§Ÿé‡‘'], keep='first')
    print(f"âœ… å»é‡å¾Œ {len(merged_df)} è¡Œæ•¸æ“š")
    
    # ç¬¬äºŒæ­¥ï¼šå°å…¥åˆ°æ•¸æ“šåº«
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    week_id = get_week_id()
    upload_date = datetime.now().strftime("%Y-%m-%d")
    
    # è¨˜éŒ„æ–°ç‰ˆæœ¬
    cursor.execute("""
        INSERT OR REPLACE INTO versions (week_id, upload_date)
        VALUES (?, ?)
    """, (week_id, upload_date))
    
    # ç²å–ç¾æœ‰æˆ¿æº IDï¼ˆåœ¨å°å…¥å‰ï¼‰
    cursor.execute("SELECT id FROM properties WHERE status = 'active'")
    existing_ids = {row[0] for row in cursor.fetchall()}
    
    # è™•ç†æ–°æ•¸æ“š
    new_ids = set()
    imported_count = 0
    
    for _, row in merged_df.iterrows():
        try:
            # æå–æ•¸æ“š
            title = str(row.get('æ¨™é¡Œ', ''))
            address = str(row.get('åœ°å€', ''))
            rent = int(row.get('ç§Ÿé‡‘', 0)) if pd.notna(row.get('ç§Ÿé‡‘', 0)) else 0
            area = float(row.get('åªæ•¸', 0)) if pd.notna(row.get('åªæ•¸', 0)) else 0
            room_type = str(row.get('æˆ¿å‹', ''))
            floor = str(row.get('æ¨“å±¤', ''))
            latitude = float(row.get('ç·¯åº¦', 0)) if pd.notna(row.get('ç·¯åº¦', 0)) else 0
            longitude = float(row.get('ç¶“åº¦', 0)) if pd.notna(row.get('ç¶“åº¦', 0)) else 0
            renovation_status = str(row.get('è£ä¿®ç‹€æ…‹', 'unknown'))
            
            # è·³éç„¡æ•ˆæ•¸æ“š
            if not address or not title or rent <= 0:
                continue
            
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("""
                SELECT id FROM properties 
                WHERE address = ? AND rent_monthly = ?
            """, (address, rent))
            
            result = cursor.fetchone()
            
            if result:
                # å·²å­˜åœ¨ï¼Œæ›´æ–°ç‹€æ…‹
                prop_id = result[0]
                new_ids.add(prop_id)
                
                cursor.execute("""
                    UPDATE properties 
                    SET status = 'active', upload_week = ?, building_type = ?
                    WHERE id = ?
                """, (week_id, 'apartment', prop_id))
            else:
                # æ–°æˆ¿æº
                first_published_date = datetime.now().strftime("%Y-%m-%d")
                
                cursor.execute("""
                    INSERT INTO properties 
                    (title, address, rent_monthly, area, room_type, floor, latitude, longitude, 
                     building_type, renovation_status, first_published_date, upload_week, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'active')
                """, (title, address, rent, area, room_type, floor, latitude, longitude,
                      'apartment', renovation_status, first_published_date, week_id))
                
                new_ids.add(cursor.lastrowid)
                imported_count += 1
        
        except Exception as e:
            continue
    
    # æ¨™è¨˜å·²åˆªé™¤çš„æˆ¿æºï¼ˆåªåœ¨æ‰€æœ‰æª”æ¡ˆéƒ½å°å…¥å®Œæˆå¾Œï¼‰
    deleted_ids = existing_ids - new_ids
    for prop_id in deleted_ids:
        cursor.execute("""
            UPDATE properties 
            SET status = 'deleted', deleted_date = ?
            WHERE id = ?
        """, (upload_date, prop_id))
    
    conn.commit()
    conn.close()
    
    print(f"âœ… CSV å°å…¥å®Œæˆï¼")
    print(f"  æ–°å¢æˆ¿æº: {imported_count}")
    print(f"  å·²åˆªé™¤æˆ¿æº: {len(deleted_ids)}")
    print(f"  ç¸½æˆ¿æºæ•¸: {len(new_ids)}")

@app.post("/api/import_data")
async def import_data(csv_filename: str):
    """
    å°å…¥ CSV æ•¸æ“šä¸¦å‰µå»ºæ–°ç‰ˆæœ¬
    """
    try:
        import pandas as pd
        
        csv_path = os.path.join(os.path.dirname(__file__), "upload", csv_filename)
        
        if not os.path.exists(csv_path):
            return {
                "status": "error",
                "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}"
            }
        
        # è®€å– CSV
        df = pd.read_csv(csv_path, encoding='utf-8-sig')
        
        # å¾æ–‡ä»¶åæå–å»ºç¯‰é¡å‹
        building_type = extract_building_type_from_filename(csv_filename)
        
        # ç”Ÿæˆé€±æ¬¡ ID
        week_id = get_week_id()
        upload_date = datetime.now().strftime("%Y-%m-%d")
        
        # é€£æ¥æ•¸æ“šåº«
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # è¨˜éŒ„æ–°ç‰ˆæœ¬
        cursor.execute("""
            INSERT OR REPLACE INTO versions (week_id, upload_date)
            VALUES (?, ?)
        """, (week_id, upload_date))
        
        # ç²å–ç¾æœ‰æˆ¿æº ID
        cursor.execute("SELECT id FROM properties WHERE status = 'active'")
        existing_ids = {row[0] for row in cursor.fetchall()}
        
        # è™•ç†æ–°æ•¸æ“š
        new_ids = set()
        for _, row in df.iterrows():
            try:
                # æå–æ•¸æ“š
                title = str(row.get('æ¨™é¡Œ', ''))
                address = str(row.get('åœ°å€', ''))
                rent = int(row.get('ç§Ÿé‡‘', 0))
                area = float(row.get('åªæ•¸', 0))
                room_type = str(row.get('æˆ¿å‹', ''))
                floor = str(row.get('æ¨“å±¤', ''))
                latitude = float(row.get('ç·¯åº¦', 0))
                longitude = float(row.get('ç¶“åº¦', 0))
                renovation_status = str(row.get('è£ä¿®ç‹€æ…‹', 'unknown'))
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                cursor.execute("""
                    SELECT id, first_published_date FROM properties 
                    WHERE address = ? AND rent_monthly = ?
                """, (address, rent))
                
                result = cursor.fetchone()
                
                if result:
                    # å·²å­˜åœ¨ï¼Œæ›´æ–°ç‹€æ…‹
                    prop_id = result[0]
                    first_published_date = result[1]
                    new_ids.add(prop_id)
                    
                    cursor.execute("""
                        UPDATE properties 
                        SET status = 'active', upload_week = ?, building_type = ?
                        WHERE id = ?
                    """, (week_id, building_type, prop_id))
                else:
                    # æ–°æˆ¿æº
                    first_published_date = datetime.now().strftime("%Y-%m-%d")
                    
                    cursor.execute("""
                        INSERT INTO properties 
                        (title, address, rent_monthly, area, room_type, floor, latitude, longitude, 
                         building_type, renovation_status, first_published_date, upload_week, status)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'active')
                    """, (title, address, rent, area, room_type, floor, latitude, longitude,
                          building_type, renovation_status, first_published_date, week_id))
                    
                    new_ids.add(cursor.lastrowid)
            
            except Exception as e:
                print(f"è™•ç†è¡Œå¤±æ•—: {e}")
                continue
        
        # æ¨™è¨˜å·²åˆªé™¤çš„æˆ¿æº
        deleted_ids = existing_ids - new_ids
        for prop_id in deleted_ids:
            cursor.execute("""
                UPDATE properties 
                SET status = 'deleted', deleted_date = ?
                WHERE id = ?
            """, (upload_date, prop_id))
        
        conn.commit()
        conn.close()
        
        return {
            "status": "success",
            "week_id": week_id,
            "upload_date": upload_date,
            "new_properties": len(new_ids),
            "deleted_properties": len(deleted_ids),
            "message": f"æˆåŠŸå°å…¥æ•¸æ“šã€‚æ–°å¢: {len(new_ids)}, åˆªé™¤: {len(deleted_ids)}"
        }
    
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

@app.get("/api/analysis_v4")
async def analysis_v4(
    address: str,
    distance_min: int = 300,
    distance_max: int = 3000,
    building_type: Optional[str] = None,
    room_type: Optional[str] = None,
    week_id: Optional[str] = None,
    lat: Optional[float] = None,
    lng: Optional[float] = None
):
    """
    åˆ†æ API v4 - æ”¯æŒç‰ˆæœ¬æŸ¥è©¢ã€ç•™ç½®æ™‚é–“è‘—è‰²å’Œå»ºç¯‰é¡å‹ç¯©é¸
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # å„ªå…ˆä½¿ç”¨å‰ç«¯å‚³ä¾†çš„åº§æ¨™ï¼Œå¦å‰‡ä½¿ç”¨é»˜èªåº§æ¨™
        if lat is not None and lng is not None and lat != 0 and lng != 0:
            query_lat = lat
            query_lon = lng
        else:
            # ä½¿ç”¨é»˜èªåº§æ¨™ï¼ˆä¸­å’Œå€ä¸­å¿ƒï¼‰
            query_lat, query_lon = 25.0288, 121.4625
        
        # æŸ¥è©¢æˆ¿æº
        query = "SELECT * FROM properties WHERE status IN ('active', 'deleted')"
        params = []
        
        if week_id:
            # å¦‚æœæŒ‡å®šé€±æ¬¡ï¼ŒåªæŸ¥è©¢è©²é€±åŠä¹‹å‰çš„æˆ¿æº
            query += " AND upload_week <= ?"
            params.append(week_id)
        
        cursor.execute(query, params)
        all_properties = cursor.fetchall()
        
        # ç²å–åˆ—å
        cursor.execute("PRAGMA table_info(properties)")
        columns = {row[1]: row[0] for row in cursor.fetchall()}
        
        # ç¯©é¸è·é›¢ç¯„åœå…§çš„æˆ¿æº
        filtered_properties = []
        for prop in all_properties:
            # è·³éæ²’æœ‰ç¶“ç·¯åº¦çš„æˆ¿æº
            if prop[columns['latitude']] is None or prop[columns['longitude']] is None:
                continue
            
            # è·³éåº§æ¨™ç•°å¸¸çš„æˆ¿æº (0,0)
            if prop[columns['latitude']] == 0.0 and prop[columns['longitude']] == 0.0:
                continue
            
            prop_dict = {
                'id': prop[columns['id']],
                'title': prop[columns['title']],
                'address': prop[columns['address']],
                'rent_monthly': prop[columns['rent_monthly']],
                'area': prop[columns['area']],
                'floor': prop[columns['floor']] if 'floor' in columns else '',
                'room_type': prop[columns['room_type']],
                'latitude': prop[columns['latitude']],
                'longitude': prop[columns['longitude']],
                'building_type': prop[columns['building_type']] if 'building_type' in columns else 'apartment',
                'renovation_status': prop[columns['renovation_status']],
                'first_published_date': prop[columns['first_published_date']],
                'upload_week': prop[columns['upload_week']],
                'status': prop[columns['status']]
            }
            
            # è¨ˆç®—è·é›¢
            distance = haversine_distance(query_lat, query_lon, prop_dict['latitude'], prop_dict['longitude'])
            
            if distance_min <= distance <= distance_max:
                # è¨ˆç®—ç•™ç½®é€±æ•¸
                weeks_since = calculate_weeks_since_published(prop_dict['first_published_date'])
                prop_dict['weeks_since_first_published'] = weeks_since
                prop_dict['distance'] = distance
                
                # æ‡‰ç”¨ç¯©é¸æ¢ä»¶
                if building_type and prop_dict['building_type'] != building_type:
                    continue
                if room_type and prop_dict['room_type'] != room_type:
                    continue
                
                filtered_properties.append(prop_dict)
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        if filtered_properties:
            rents = [p['rent_monthly'] for p in filtered_properties if p['status'] == 'active']
            areas = [p['area'] for p in filtered_properties if p['status'] == 'active']
            
            summary = {
                'total_properties': len(filtered_properties),
                'active_properties': len([p for p in filtered_properties if p['status'] == 'active']),
                'deleted_properties': len([p for p in filtered_properties if p['status'] == 'deleted']),
                'new_properties': len([p for p in filtered_properties if p['weeks_since_first_published'] == 0]),
                'avg_rent_all': sum(rents) / len(rents) if rents else 0,
                'min_rent': min(rents) if rents else 0,
                'max_rent': max(rents) if rents else 0,
                'avg_area': sum(areas) / len(areas) if areas else 0,
            }
        else:
            summary = {
                'total_properties': 0,
                'active_properties': 0,
                'deleted_properties': 0,
                'new_properties': 0,
                'avg_rent_all': 0,
                'min_rent': 0,
                'max_rent': 0,
                'avg_area': 0,
            }
        
        # æˆ¿å‹åˆ†æ
        room_type_analysis = {}
        for prop in filtered_properties:
            if prop['status'] == 'active':
                rt = prop['room_type']
                room_type_analysis[rt] = room_type_analysis.get(rt, 0) + 1
        
        room_type_analysis = [
            {'room_type': k, 'count': v}
            for k, v in sorted(room_type_analysis.items(), key=lambda x: x[1], reverse=True)
        ]
        
        conn.close()
        
        return {
            'status': 'success',
            'query': {
                'address': address,
                'coordinates': {'latitude': query_lat, 'longitude': query_lon},
                'distance_range': {'min': distance_min, 'max': distance_max},
                'week_id': week_id or 'current'
            },
            'summary': summary,
            'properties': filtered_properties,
            'room_type_analysis': room_type_analysis
        }
    
    except Exception as e:
        return {
            'status': 'error',
            'message': str(e)
        }

# ============ ç®¡ç†å“¡ API ============

# ç®¡ç†å“¡å¯†ç¢¼ï¼ˆMVP æ¸¬è©¦ç”¨ï¼‰
ADMIN_PASSWORD = "1234"

class ResetRequest(BaseModel):
    password: str
    confirm: bool = False

@app.post("/api/admin/reset-database")
async def reset_database(request: ResetRequest):
    """
    æ¸…ç©ºæ•¸æ“šåº« APIï¼ˆéœ€è¦å¯†ç¢¼é©—è­‰ï¼‰
    
    ä½¿ç”¨æ–¹æ³•ï¼š
    POST /api/admin/reset-database
    Body: {"password": "1234", "confirm": true}
    """
    # é©—è­‰å¯†ç¢¼
    if request.password != ADMIN_PASSWORD:
        raise HTTPException(status_code=403, detail="å¯†ç¢¼éŒ¯èª¤")
    
    # é©—è­‰ç¢ºèªåƒæ•¸
    if not request.confirm:
        raise HTTPException(status_code=400, detail="è«‹è¨­ç½® confirm=true ä»¥ç¢ºèªæ¸…ç©ºæ“ä½œ")
    
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # çµ±è¨ˆåˆªé™¤å‰çš„æ•¸æ“š
        cursor.execute("SELECT COUNT(*) FROM properties")
        properties_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM versions")
        versions_count = cursor.fetchone()[0]
        
        # æ¸…ç©ºæ‰€æœ‰è¡¨
        cursor.execute("DELETE FROM properties")
        cursor.execute("DELETE FROM versions")
        
        # é‡ç½®è‡ªå¢ ID
        cursor.execute("DELETE FROM sqlite_sequence WHERE name='properties'")
        cursor.execute("DELETE FROM sqlite_sequence WHERE name='versions'")
        
        conn.commit()
        conn.close()
        
        return {
            "status": "success",
            "message": "æ•¸æ“šåº«å·²æ¸…ç©º",
            "deleted": {
                "properties": properties_count,
                "versions": versions_count
            },
            "timestamp": datetime.now().isoformat()
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºå¤±æ•—: {str(e)}")

@app.get("/api/admin/database-status")
async def database_status():
    """
    æŸ¥çœ‹æ•¸æ“šåº«ç‹€æ…‹ï¼ˆä¸éœ€è¦å¯†ç¢¼ï¼‰
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # çµ±è¨ˆæˆ¿æºæ•¸é‡
        cursor.execute("SELECT COUNT(*) FROM properties")
        total_properties = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM properties WHERE status='active'")
        active_properties = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM properties WHERE status='deleted'")
        deleted_properties = cursor.fetchone()[0]
        
        # çµ±è¨ˆç‰ˆæœ¬æ•¸é‡
        cursor.execute("SELECT COUNT(*) FROM versions")
        versions_count = cursor.fetchone()[0]
        
        # ç²å–æ‰€æœ‰ç‰ˆæœ¬
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            "status": "success",
            "database": {
                "total_properties": total_properties,
                "active_properties": active_properties,
                "deleted_properties": deleted_properties,
                "versions_count": versions_count,
                "versions": versions
            },
            "timestamp": datetime.now().isoformat()
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŸ¥è©¢å¤±æ•—: {str(e)}")

# ============ éœæ…‹æ–‡ä»¶ ============

# æŒ‚è¼‰éœæ…‹æ–‡ä»¶ï¼ˆå¿…é ˆåœ¨æ‰€æœ‰ API è·¯ç”±ä¹‹å¾Œï¼‰
app.mount("/", StaticFiles(directory=os.path.dirname(__file__), html=True), name="static")

# ============ å•Ÿå‹• ============

if __name__ == "__main__":
    init_database()
    
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
