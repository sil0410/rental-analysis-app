"""
ç§Ÿå±‹è¡Œæƒ…åˆ†æç³»çµ± - ç‰ˆæœ¬æ§åˆ¶ API v7.0
æ”¯æŒå››è±¡é™åˆ†é¡ï¼ˆå»ºç‰©é¡å‹ x æˆ¿å‹å¤§é¡ï¼‰æŒ‰éœ€è¼‰å…¥ CSV
æ”¯æŒ Google Drive åˆ†å±¤è³‡æ–™å¤¾ç®¡ç†
å„ªåŒ–æ•ˆèƒ½ï¼šåªè¼‰å…¥æŒ‡å®šç¯©é¸æ¢ä»¶çš„æ•¸æ“š
"""

import sqlite3
import json
import os
import re
from datetime import datetime, timedelta
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional
import math
import pandas as pd
from io import BytesIO

# åˆå§‹åŒ– FastAPI
app = FastAPI(title="ç§Ÿå±‹è¡Œæƒ…åˆ†æ API v7.0")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•¸æ“šåº«è·¯å¾‘
DB_PATH = os.path.join(os.path.dirname(__file__), "rental.db")

# Upload è³‡æ–™å¤¾è·¯å¾‘
UPLOAD_DIR = None

# ============ Google Drive é…ç½® ============
GOOGLE_DRIVE_FOLDER_NAME = "ç§Ÿå±‹æ•¸æ“š"
drive_service = None
drive_folder_id = None
drive_available = False

def init_google_drive():
    """åˆå§‹åŒ– Google Drive APIï¼ˆå¯é¸åŠŸèƒ½ï¼‰"""
    global drive_service, drive_folder_id, drive_available
    
    try:
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– Google Drive é‡‘é‘°
        key_json_str = os.getenv('GOOGLE_DRIVE_KEY_JSON')
        
        if not key_json_str:
            print("â„¹ï¸ Google Drive æœªé…ç½®ï¼ˆç’°å¢ƒè®Šæ•¸ GOOGLE_DRIVE_KEY_JSON æœªè¨­å®šï¼‰")
            print("   ç³»çµ±å°‡ä½¿ç”¨æœ¬åœ° upload è³‡æ–™å¤¾")
            return False
        
        # å»¶é²å°å…¥ Google Drive ç›¸é—œæ¨¡çµ„
        try:
            from google.oauth2.service_account import Credentials
            from googleapiclient.discovery import build
        except ImportError:
            print("âš ï¸ Google Drive API æ¨¡çµ„æœªå®‰è£")
            print("   è«‹åŸ·è¡Œ: pip install google-auth google-api-python-client")
            return False
        
        # è§£æ JSON é‡‘é‘°
        try:
            key_dict = json.loads(key_json_str)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ ç„¡æ³•è§£æ Google Drive é‡‘é‘° JSONï¼š{e}")
            return False
        
        # å»ºç«‹èªè­‰
        credentials = Credentials.from_service_account_info(
            key_dict,
            scopes=['https://www.googleapis.com/auth/drive']
        )
        
        drive_service = build('drive', 'v3', credentials=credentials)
        
        # æŸ¥æ‰¾ã€Œç§Ÿå±‹æ•¸æ“šã€è³‡æ–™å¤¾
        results = drive_service.files().list(
            q=f"name='{GOOGLE_DRIVE_FOLDER_NAME}' and mimeType='application/vnd.google-apps.folder' and trashed=false",
            spaces='drive',
            fields='files(id, name)',
            pageSize=1
        ).execute()
        
        files = results.get('files', [])
        if files:
            drive_folder_id = files[0]['id']
            drive_available = True
            print(f"âœ“ Google Drive é€£æ¥æˆåŠŸ")
            print(f"  - è³‡æ–™å¤¾: {GOOGLE_DRIVE_FOLDER_NAME}")
            print(f"  - ID: {drive_folder_id}")
            return True
        else:
            print(f"âš ï¸ æ‰¾ä¸åˆ° Google Drive ä¸­çš„ã€Œ{GOOGLE_DRIVE_FOLDER_NAME}ã€è³‡æ–™å¤¾")
            return False
            
    except Exception as e:
        print(f"âš ï¸ Google Drive åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        return False

def get_csv_from_drive(city: str, district: str, building_type: str, property_category: str, week_id: str) -> Optional[pd.DataFrame]:
    """å¾ Google Drive è®€å–æŒ‡å®šçš„ CSV æ–‡ä»¶"""
    if not drive_available or not drive_service or not drive_folder_id:
        return None
    
    try:
        from googleapiclient.http import MediaIoBaseDownload
        
        # æ§‹å»ºæ–‡ä»¶å
        filename = f"{building_type}_{property_category}_{week_id}.csv"
        
        # æŸ¥æ‰¾åŸå¸‚è³‡æ–™å¤¾
        city_results = drive_service.files().list(
            q=f"name='{city}' and mimeType='application/vnd.google-apps.folder' and '{drive_folder_id}' in parents and trashed=false",
            spaces='drive',
            fields='files(id)',
            pageSize=1
        ).execute()
        
        city_files = city_results.get('files', [])
        if not city_files:
            return None
        
        city_folder_id = city_files[0]['id']
        
        # æŸ¥æ‰¾å€åŸŸè³‡æ–™å¤¾
        district_results = drive_service.files().list(
            q=f"name='{district}' and mimeType='application/vnd.google-apps.folder' and '{city_folder_id}' in parents and trashed=false",
            spaces='drive',
            fields='files(id)',
            pageSize=1
        ).execute()
        
        district_files = district_results.get('files', [])
        if not district_files:
            return None
        
        district_folder_id = district_files[0]['id']
        
        # æŸ¥æ‰¾ CSV æ–‡ä»¶
        csv_results = drive_service.files().list(
            q=f"name='{filename}' and '{district_folder_id}' in parents and trashed=false",
            spaces='drive',
            fields='files(id)',
            pageSize=1
        ).execute()
        
        csv_files = csv_results.get('files', [])
        if not csv_files:
            return None
        
        csv_file_id = csv_files[0]['id']
        
        # ä¸‹è¼‰ CSV æ–‡ä»¶
        request = drive_service.files().get_media(fileId=csv_file_id)
        file_content = BytesIO()
        downloader = MediaIoBaseDownload(file_content, request)
        
        done = False
        while not done:
            status, done = downloader.next_chunk()
        
        file_content.seek(0)
        df = pd.read_csv(file_content, encoding='utf-8-sig')
        
        print(f"  âœ“ å¾ Google Drive è¼‰å…¥: {city}/{district}/{filename}")
        return df
        
    except Exception as e:
        print(f"  âš ï¸ å¾ Google Drive è®€å– CSV å¤±æ•—ï¼š{e}")
        return None

def list_google_drive_files(folder_id: str, path: str = "") -> list:
    """éè¿´åˆ—å‡º Google Drive è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ CSV æª”æ¡ˆ"""
    if not drive_available or not drive_service:
        return []
    
    files_found = []
    
    try:
        # åˆ—å‡ºè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰é …ç›®
        results = drive_service.files().list(
            q=f"'{folder_id}' in parents and trashed=false",
            spaces='drive',
            fields='files(id, name, mimeType)',
            pageSize=1000
        ).execute()
        
        items = results.get('files', [])
        
        for item in items:
            item_name = item['name']
            item_id = item['id']
            item_type = item['mimeType']
            current_path = f"{path}/{item_name}" if path else item_name
            
            if item_type == 'application/vnd.google-apps.folder':
                # éè¿´é€²å…¥å­è³‡æ–™å¤¾
                sub_files = list_google_drive_files(item_id, current_path)
                files_found.extend(sub_files)
            elif item_name.endswith('.csv'):
                # æ‰¾åˆ° CSV æª”æ¡ˆ
                files_found.append({
                    'id': item_id,
                    'name': item_name,
                    'path': current_path
                })
        
        return files_found
        
    except Exception as e:
        print(f"âš ï¸ åˆ—å‡º Google Drive è³‡æ–™å¤¾å¤±æ•— ({path}): {e}")
        return []

# ============ æœ¬åœ°æ–‡ä»¶ç³»çµ± ============

def get_upload_dir():
    global UPLOAD_DIR
    if UPLOAD_DIR:
        return UPLOAD_DIR
    
    possible_paths = [
        os.path.join(os.path.dirname(__file__), "upload"),
        "/app/upload",
        "./upload",
        os.path.join(os.getcwd(), "upload")
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            UPLOAD_DIR = path
            return UPLOAD_DIR
    
    UPLOAD_DIR = possible_paths[0]
    if not os.path.exists(UPLOAD_DIR):
        os.makedirs(UPLOAD_DIR)
    return UPLOAD_DIR

# ============ æ‡‰ç”¨å•Ÿå‹•äº‹ä»¶ ============

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚åˆå§‹åŒ–æ•¸æ“šåº«ä¸¦æƒæå¯ç”¨çš„ CSV æ–‡ä»¶"""
    try:
        init_database()
        init_google_drive()  # å˜—è©¦åˆå§‹åŒ– Google Driveï¼ˆå¯é¸ï¼‰
        scan_available_csv_files()
    except Exception as e:
        print(f"âš ï¸ å•Ÿå‹•äº‹ä»¶éŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()

# ============ æ•¸æ“šåº«åˆå§‹åŒ– ============

def init_database():
    """åˆå§‹åŒ–æ•¸æ“šåº«"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # ç‰ˆæœ¬è¡¨
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS versions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            week_id TEXT UNIQUE NOT NULL,
            upload_date TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # CSV æ–‡ä»¶ç´¢å¼•è¡¨ï¼ˆè¨˜éŒ„å¯ç”¨çš„ CSV æ–‡ä»¶ï¼‰
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS csv_index (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            filename TEXT UNIQUE NOT NULL,
            city TEXT,
            district TEXT,
            building_type TEXT,
            property_category TEXT,
            week_id TEXT,
            record_count INTEGER DEFAULT 0,
            source TEXT DEFAULT 'local',
            file_id TEXT,
            last_scanned TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # å˜—è©¦æ–°å¢ file_id æ¬„ä½ï¼ˆå¦‚æœè¡¨å·²å­˜åœ¨ä½†æ²’æœ‰æ­¤æ¬„ä½ï¼‰
    try:
        cursor.execute("ALTER TABLE csv_index ADD COLUMN file_id TEXT")
    except:
        pass  # æ¬„ä½å·²å­˜åœ¨
    
    conn.commit()
    conn.close()

# ============ å·¥å…·å‡½æ•¸ ============

def get_week_id(date: datetime = None) -> str:
    if date is None:
        date = datetime.now()
    year = date.year % 100
    week = date.isocalendar()[1]
    return f"{year:02d}{week:02d}"

def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    R = 6371000
    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    delta_lat = math.radians(lat2 - lat1)
    delta_lon = math.radians(lon2 - lon1)
    a = math.sin(delta_lat/2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon/2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))
    return R * c

def calculate_weeks_since_published(first_published_date: str) -> int:
    if not first_published_date:
        return 0
    try:
        first_date = datetime.strptime(first_published_date, "%Y-%m-%d")
        now = datetime.now()
        delta = now - first_date
        weeks = delta.days // 7
        return max(0, weeks)
    except:
        return 0

def parse_dms_coordinate(coord_str: str):
    """è§£æåº¦åˆ†ç§’æ ¼å¼çš„åº§æ¨™å­—ä¸²"""
    if not coord_str or coord_str == 'nan':
        return 0, 0
    
    try:
        coord_str = str(coord_str).strip()
        pattern = r"(\d+)Â°(\d+)'(\d+(?:\.\d+)?)\"([NSEW])"
        matches = re.findall(pattern, coord_str)
        
        if len(matches) >= 2:
            lat_match = None
            lng_match = None
            
            for match in matches:
                deg, min_, sec, direction = match
                if direction in ['N', 'S']:
                    lat_match = match
                elif direction in ['E', 'W']:
                    lng_match = match
            
            if lat_match and lng_match:
                lat_deg, lat_min, lat_sec, lat_dir = lat_match
                lat = float(lat_deg) + float(lat_min)/60 + float(lat_sec)/3600
                if lat_dir == 'S':
                    lat = -lat
                
                lng_deg, lng_min, lng_sec, lng_dir = lng_match
                lng = float(lng_deg) + float(lng_min)/60 + float(lng_sec)/3600
                if lng_dir == 'W':
                    lng = -lng
                
                return lat, lng
    except Exception as e:
        pass
    
    return 0, 0

def parse_csv_filename(filename: str) -> dict:
    """è§£æ CSV æ–‡ä»¶åï¼Œæå–ç›¸é—œä¿¡æ¯"""
    result = {
        'city': '',
        'district': '',
        'building_type': '',
        'property_category': '',
        'week_id': ''
    }
    
    name = filename.replace('.csv', '')
    
    week_match = re.search(r'_(\d{4})(?:_merged)?$', name)
    if week_match:
        result['week_id'] = week_match.group(1)
    
    if 'é›»æ¢¯å¤§æ¨“' in filename or 'é›»æ¢¯' in filename:
        result['building_type'] = 'building'
    elif 'å…¬å¯“' in filename:
        result['building_type'] = 'apartment'
    
    if 'å¥—æˆ¿' in filename or 'ç¨ç«‹å¥—æˆ¿' in filename:
        result['property_category'] = 'å¥—æˆ¿'
    elif 'ä½å®¶' in filename or 'æ•´å±¤ä½å®¶' in filename:
        result['property_category'] = 'ä½å®¶'
    
    districts = [
        'æ¿æ©‹å€', 'ä¸‰é‡å€', 'ä¸­å’Œå€', 'æ°¸å’Œå€', 'æ–°èŠå€', 'æ–°åº—å€', 'åœŸåŸå€',
        'è˜†æ´²å€', 'æ¨¹æ—å€', 'æ±æ­¢å€', 'é¶¯æ­Œå€', 'ä¸‰å³½å€', 'æ·¡æ°´å€', 'ç‘èŠ³å€',
        'äº”è‚¡å€', 'æ³°å±±å€', 'æ—å£å€', 'æ·±å‘å€', 'çŸ³ç¢‡å€', 'åªæ—å€', 'ä¸‰èŠå€',
        'çŸ³é–€å€', 'å…«é‡Œå€', 'å¹³æºªå€', 'é›™æºªå€', 'è²¢å¯®å€', 'é‡‘å±±å€', 'è¬é‡Œå€',
        'çƒä¾†å€', 'å¤§å®‰å€', 'ä¿¡ç¾©å€', 'ä¸­å±±å€', 'æ¾å±±å€', 'å—æ¸¯å€', 'å…§æ¹–å€'
    ]
    
    for district in districts:
        if district in filename:
            result['district'] = district
            result['city'] = 'æ–°åŒ—å¸‚'
            break
    
    if filename.startswith('æ–°åŒ—å¸‚'):
        result['city'] = 'æ–°åŒ—å¸‚'
    elif filename.startswith('è‡ºåŒ—å¸‚') or filename.startswith('å°åŒ—å¸‚'):
        result['city'] = 'è‡ºåŒ—å¸‚'
    elif filename.startswith('åŸºéš†å¸‚'):
        result['city'] = 'åŸºéš†å¸‚'
    elif filename.startswith('æ¡ƒåœ’å¸‚'):
        result['city'] = 'æ¡ƒåœ’å¸‚'
    
    return result

def scan_available_csv_files():
    """æƒæ upload è³‡æ–™å¤¾å’Œ Google Drive ä¸­çš„ CSV æ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•"""
    upload_dir = get_upload_dir()
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    # æ¸…ç©ºèˆŠç´¢å¼•
    cursor.execute("DELETE FROM csv_index")
    
    week_ids = set()
    total_files = 0
    
    # === æƒææœ¬åœ° upload è³‡æ–™å¤¾ ===
    if os.path.exists(upload_dir):
        csv_files = [f for f in os.listdir(upload_dir) if f.endswith('.csv')]
        print(f"ğŸ“ æœ¬åœ°æƒæåˆ° {len(csv_files)} å€‹ CSV æª”æ¡ˆ")
        
        for csv_filename in csv_files:
            try:
                info = parse_csv_filename(csv_filename)
                
                csv_path = os.path.join(upload_dir, csv_filename)
                try:
                    record_count = sum(1 for _ in open(csv_path, encoding='utf-8-sig')) - 1
                except:
                    record_count = 0
                
                cursor.execute("""
                    INSERT OR REPLACE INTO csv_index 
                    (filename, city, district, building_type, property_category, week_id, record_count, source, last_scanned)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (csv_filename, info['city'], info['district'], info['building_type'], 
                      info['property_category'], info['week_id'], record_count, 'local', datetime.now().isoformat()))
                
                if info['week_id']:
                    week_ids.add(info['week_id'])
                
                total_files += 1
                print(f"  âœ“ [local] {csv_filename}: {info['city']}/{info['district']} / {info['building_type']} / {info['property_category']} / {info['week_id']}")
            
            except Exception as e:
                print(f"  âš ï¸ {csv_filename} è™•ç†å¤±æ•—: {e}")
    else:
        print(f"âš ï¸ Upload è³‡æ–™å¤¾ä¸å­˜åœ¨: {upload_dir}")
    
    # === æƒæ Google Drive ===
    print(f"ğŸ“ Google Drive ç‹€æ…‹: available={drive_available}, folder_id={drive_folder_id}")
    if drive_available and drive_folder_id:
        print(f"ğŸ“ é–‹å§‹æƒæ Google Drive...")
        try:
            drive_files = list_google_drive_files(drive_folder_id)
            print(f"ğŸ“ Google Drive æƒæåˆ° {len(drive_files)} å€‹ CSV æª”æ¡ˆ")
            
            for file_info in drive_files:
                try:
                    filename = file_info['name']
                    file_path = file_info['path']
                    file_id = file_info['id']
                    
                    # å¾è·¯å¾‘è§£æåŸå¸‚å’Œå€åŸŸ
                    # è·¯å¾‘æ ¼å¼: "ç¸£å¸‚/å€åŸŸ/æª”æ¡ˆå.csv"
                    path_parts = file_path.split('/')
                    city = ''
                    district = ''
                    
                    if len(path_parts) >= 3:
                        city = path_parts[0]
                        district = path_parts[1]
                    elif len(path_parts) == 2:
                        city = path_parts[0]
                    
                    info = parse_csv_filename(filename)
                    
                    # å¦‚æœå¾è·¯å¾‘è§£æåˆ°äº†åŸå¸‚å’Œå€åŸŸï¼Œå„ªå…ˆä½¿ç”¨è·¯å¾‘ä¸­çš„è³‡è¨Š
                    if city:
                        info['city'] = city
                    if district:
                        info['district'] = district
                    
                    cursor.execute("""
                        INSERT OR REPLACE INTO csv_index 
                        (filename, city, district, building_type, property_category, week_id, record_count, source, file_id, last_scanned)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (filename, info['city'], info['district'], info['building_type'], 
                          info['property_category'], info['week_id'], 0, 'google_drive', file_id, datetime.now().isoformat()))
                    
                    if info['week_id']:
                        week_ids.add(info['week_id'])
                    
                    total_files += 1
                    print(f"  âœ“ [drive] {file_path}: {info['city']}/{info['district']} / {info['building_type']} / {info['property_category']} / {info['week_id']}")
                
                except Exception as e:
                    print(f"  âš ï¸ {file_info['name']} è™•ç†å¤±æ•—: {e}")
                    import traceback
                    traceback.print_exc()
        except Exception as e:
            print(f"âš ï¸ Google Drive æƒæéç¨‹å‡ºéŒ¯: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"â„¹ï¸ Google Drive æœªé…ç½®æˆ–ä¸å¯ç”¨ (available={drive_available}, folder_id={drive_folder_id})")
    
    # === æ›´æ–°ç‰ˆæœ¬è¨˜éŒ„ ===
    for week_id in week_ids:
        cursor.execute("""
            INSERT OR REPLACE INTO versions (week_id, upload_date)
            VALUES (?, ?)
        """, (week_id, datetime.now().strftime("%Y-%m-%d")))
    
    conn.commit()
    conn.close()
    
    print(f"âœ“ ç´¢å¼•å»ºç«‹å®Œæˆ: {total_files} å€‹æ–‡ä»¶, {len(week_ids)} å€‹é€±æ¬¡ç‰ˆæœ¬")

def load_csv_data(city: str, district: str, building_type: str, property_category: str, week_id: str) -> List[dict]:
    """
    æŒ‰éœ€è¼‰å…¥ CSV æ•¸æ“š
    å„ªå…ˆå¾ Google Drive è¼‰å…¥ï¼Œæ¬¡ä¹‹å¾æœ¬åœ° upload è³‡æ–™å¤¾
    """
    upload_dir = get_upload_dir()
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    query = "SELECT filename FROM csv_index WHERE 1=1"
    params = []
    
    if district:
        query += " AND district = ?"
        params.append(district)
    
    if building_type and building_type != 'å…¨éƒ¨':
        bt = 'apartment' if building_type == 'å…¬å¯“' else 'building' if building_type == 'é›»æ¢¯å¤§æ¨“' else building_type
        query += " AND building_type = ?"
        params.append(bt)
    
    if property_category and property_category != 'å…¨éƒ¨':
        query += " AND property_category = ?"
        params.append(property_category)
    
    if week_id:
        query += " AND week_id = ?"
        params.append(week_id)
    
    cursor.execute(query, params)
    csv_files = [row[0] for row in cursor.fetchall()]
    conn.close()
    
    print(f"ğŸ“‚ è¼‰å…¥ CSV: district={district}, building={building_type}, category={property_category}, week={week_id}")
    print(f"   æ‰¾åˆ° {len(csv_files)} å€‹åŒ¹é…çš„ CSV æ–‡ä»¶: {csv_files}")
    
    all_properties = []
    
    # å˜—è©¦å¾ Google Drive è¼‰å…¥
    if drive_available and district and week_id:
        building_types_to_load = []
        if building_type == 'å…¨éƒ¨' or not building_type:
            building_types_to_load = ['å…¬å¯“', 'é›»æ¢¯å¤§æ¨“']
        else:
            building_types_to_load = [building_type]
        
        categories_to_load = []
        if property_category == 'å…¨éƒ¨' or not property_category:
            categories_to_load = ['å¥—æˆ¿', 'ä½å®¶']
        else:
            categories_to_load = [property_category]
        
        for bt in building_types_to_load:
            for cat in categories_to_load:
                df = get_csv_from_drive(city, district, bt, cat, week_id)
                if df is not None:
                    properties = process_dataframe(df, city, district, bt, cat, week_id)
                    all_properties.extend(properties)
    
    # å¦‚æœ Google Drive æ²’æœ‰æ•¸æ“šï¼Œå¾æœ¬åœ°è¼‰å…¥
    if not all_properties:
        for csv_filename in csv_files:
            try:
                csv_path = os.path.join(upload_dir, csv_filename)
                df = pd.read_csv(csv_path, encoding='utf-8-sig')
                
                file_info = parse_csv_filename(csv_filename)
                
                properties = process_dataframe(
                    df, 
                    file_info['city'], 
                    file_info['district'], 
                    file_info['building_type'], 
                    file_info['property_category'], 
                    file_info['week_id']
                )
                all_properties.extend(properties)
            
            except Exception as e:
                print(f"  âš ï¸ {csv_filename} è®€å–å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
    
    print(f"   è¼‰å…¥å®Œæˆ: {len(all_properties)} ç­†æˆ¿æº")
    return all_properties

def process_dataframe(df: pd.DataFrame, city: str, district: str, building_type: str, property_category: str, week_id: str) -> List[dict]:
    """è™•ç† DataFrame ä¸¦è½‰æ›ç‚ºæˆ¿æºåˆ—è¡¨"""
    properties = []
    
    for _, row in df.iterrows():
        property_id = row.get('æ¡ˆä»¶ç·¨è™Ÿ', '')
        if pd.isna(property_id) or not property_id:
            continue
        property_id = str(int(property_id) if isinstance(property_id, float) else property_id)
        
        title = str(row.get('æ¨™é¡Œ', ''))
        
        raw_address = str(row.get('åœ°å€', ''))
        if city and not raw_address.startswith(city):
            raw_address = city + raw_address
        if district and district not in raw_address:
            raw_address = raw_address.replace(city, city + district)
        address = raw_address
        
        rent = row.get('ç§Ÿé‡‘', 0)
        if pd.isna(rent):
            rent = 0
        rent = int(rent)
        
        area = row.get('åªæ•¸', row.get('å¡æ•¸', 0))
        if pd.isna(area):
            area = 0
        area = float(area)
        
        room_type = str(row.get('æˆ¿å‹', ''))
        if room_type == 'nan':
            room_type = ''
        
        floor = str(row.get('æ¨“å±¤', ''))
        if floor == 'nan':
            floor = ''
        
        building_type_val = building_type or 'unknown'
        property_category_val = property_category or ''
        
        latitude = 0
        longitude = 0
        
        if 'ç·¯åº¦' in df.columns and 'ç¶“åº¦' in df.columns:
            lat_val = row.get('ç·¯åº¦', 0)
            lng_val = row.get('ç¶“åº¦', 0)
            if not pd.isna(lat_val) and not pd.isna(lng_val):
                latitude = float(lat_val)
                longitude = float(lng_val)
        
        if latitude == 0 and longitude == 0 and 'åº§æ¨™' in df.columns:
            coord_str = row.get('åº§æ¨™', '')
            if not pd.isna(coord_str):
                latitude, longitude = parse_dms_coordinate(str(coord_str))
        
        prop_week_id = row.get('é€±æ¬¡', row.get('å¹´é€±', ''))
        if pd.isna(prop_week_id) or not prop_week_id:
            prop_week_id = week_id or get_week_id()
        prop_week_id = str(prop_week_id)
        if prop_week_id.endswith('.0'):
            prop_week_id = prop_week_id[:-2]
        
        if not address or rent <= 0:
            continue
        
        properties.append({
            'property_id': property_id,
            'title': title,
            'address': address,
            'rent_monthly': rent,
            'area': area,
            'room_type': room_type,
            'floor': floor,
            'latitude': latitude,
            'longitude': longitude,
            'building_type': building_type_val,
            'property_category': property_category_val,
            'upload_week': prop_week_id,
            'status': 'active'
        })
    
    return properties

# ============ API ç«¯é» ============

@app.get("/api/versions")
async def get_versions():
    """ç²å–æ‰€æœ‰å¯ç”¨çš„é€±æ¬¡ç‰ˆæœ¬"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        conn.close()
        return {"status": "success", "versions": versions, "count": len(versions)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/available-filters")
async def get_available_filters():
    """ç²å–å¯ç”¨çš„ç¯©é¸é¸é …ï¼ˆåŸºæ–¼ç¾æœ‰ CSV æ–‡ä»¶ï¼‰"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute("SELECT DISTINCT city, district FROM csv_index WHERE district != '' ORDER BY city, district")
        districts = [{"city": row[0], "district": row[1]} for row in cursor.fetchall()]
        
        cursor.execute("SELECT DISTINCT building_type FROM csv_index WHERE building_type != ''")
        building_types = [row[0] for row in cursor.fetchall()]
        
        cursor.execute("SELECT DISTINCT property_category FROM csv_index WHERE property_category != ''")
        property_categories = [row[0] for row in cursor.fetchall()]
        
        cursor.execute("SELECT DISTINCT week_id FROM csv_index WHERE week_id != '' ORDER BY week_id DESC")
        week_ids = [row[0] for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            "status": "success",
            "filters": {
                "districts": districts,
                "building_types": building_types,
                "property_categories": property_categories,
                "week_ids": week_ids
            },
            "google_drive_available": drive_available
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis_v4")
async def analysis_v4(
    address: str,
    city: Optional[str] = None,
    district: Optional[str] = None,
    distance_min: int = 0,
    distance_max: int = 5000,
    building_type: Optional[str] = None,
    property_category: Optional[str] = None,
    room_type: Optional[str] = None,
    week_id: Optional[str] = None,
    lat: Optional[float] = None,
    lng: Optional[float] = None
):
    """åˆ†æ API - æŒ‰éœ€è¼‰å…¥æŒ‡å®šæ¢ä»¶çš„æ•¸æ“š"""
    try:
        if lat is not None and lng is not None and lat != 0 and lng != 0:
            query_lat, query_lon = lat, lng
        else:
            query_lat, query_lon = 25.0288, 121.4625
        
        if not district:
            districts = [
                'æ¿æ©‹å€', 'ä¸‰é‡å€', 'ä¸­å’Œå€', 'æ°¸å’Œå€', 'æ–°èŠå€', 'æ–°åº—å€', 'åœŸåŸå€',
                'è˜†æ´²å€', 'æ¨¹æ—å€', 'æ±æ­¢å€', 'é¶¯æ­Œå€', 'ä¸‰å³½å€', 'æ·¡æ°´å€', 'ç‘èŠ³å€',
                'äº”è‚¡å€', 'æ³°å±±å€', 'æ—å£å€', 'æ·±å‘å€', 'çŸ³ç¢‡å€', 'åªæ—å€', 'ä¸‰èŠå€',
                'çŸ³é–€å€', 'å…«é‡Œå€', 'å¹³æºªå€', 'é›™æºªå€', 'è²¢å¯®å€', 'é‡‘å±±å€', 'è¬é‡Œå€',
                'çƒä¾†å€', 'å¤§å®‰å€', 'ä¿¡ç¾©å€', 'ä¸­å±±å€', 'æ¾å±±å€', 'å—æ¸¯å€', 'å…§æ¹–å€'
            ]
            for d in districts:
                if d in address:
                    district = d
                    break
        
        load_category = None
        if room_type == 'å¥—æˆ¿':
            load_category = 'å¥—æˆ¿'
        elif room_type in ['2æˆ¿', '3æˆ¿', '3æˆ¿ä»¥ä¸Š']:
            load_category = 'ä½å®¶'
        elif property_category:
            load_category = property_category
        
        # æ ¹æ“šå€åŸŸè‡ªå‹•åˆ¤æ–·åŸå¸‚ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        if not city:
            taipei_districts = ['ä¸­æ­£å€', 'å¤§åŒå€', 'ä¸­å±±å€', 'æ¾å±±å€', 'å¤§å®‰å€', 'è¬è¯å€', 'ä¿¡ç¾©å€', 'å£«æ—å€', 'åŒ—æŠ•å€', 'å…§æ¹–å€', 'å—æ¸¯å€', 'æ–‡å±±å€']
            if district in taipei_districts:
                city = 'å°åŒ—å¸‚'
            else:
                city = 'æ–°åŒ—å¸‚'
        
        all_properties = load_csv_data(
            city=city,
            district=district,
            building_type=building_type,
            property_category=load_category,
            week_id=week_id
        )
        
        filtered_properties = []
        for prop in all_properties:
            if prop['latitude'] == 0 and prop['longitude'] == 0:
                continue
            
            distance = haversine_distance(query_lat, query_lon, prop['latitude'], prop['longitude'])
            
            if distance_min <= distance <= distance_max:
                prop['distance'] = distance
                
                if room_type and room_type != 'å…¨éƒ¨':
                    if room_type == 'å¥—æˆ¿':
                        if prop.get('property_category') != 'å¥—æˆ¿' and 'å¥—æˆ¿' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '2æˆ¿':
                        if '2' not in prop.get('room_type', '') and 'å…©' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '3æˆ¿':
                        if '3' not in prop.get('room_type', '') and 'ä¸‰' not in prop.get('room_type', ''):
                            continue
                    elif room_type == '3æˆ¿ä»¥ä¸Š':
                        rt = prop.get('room_type', '')
                        has_large = any(str(n) in rt for n in range(4, 10)) or any(c in rt for c in ['å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹'])
                        if not has_large:
                            continue
                
                filtered_properties.append(prop)
        
        active_properties = [p for p in filtered_properties if p['status'] == 'active']
        
        if active_properties:
            avg_rent = sum(p['rent_monthly'] for p in active_properties) / len(active_properties)
            min_rent = min(p['rent_monthly'] for p in active_properties)
            max_rent = max(p['rent_monthly'] for p in active_properties)
            avg_area = sum(p['area'] for p in active_properties if p['area'] > 0) / max(1, len([p for p in active_properties if p['area'] > 0]))
        else:
            avg_rent = min_rent = max_rent = avg_area = 0
        
        room_type_counts = {}
        for p in active_properties:
            rt = p['room_type'] or 'æœªçŸ¥'
            room_type_counts[rt] = room_type_counts.get(rt, 0) + 1
        
        room_type_analysis = [{"room_type": rt, "count": count} for rt, count in sorted(room_type_counts.items(), key=lambda x: -x[1])]
        
        return {
            "status": "success",
            "query": {
                "address": address,
                "district": district,
                "coordinates": {"latitude": query_lat, "longitude": query_lon},
                "distance_range": {"min": distance_min, "max": distance_max},
                "building_type": building_type,
                "property_category": load_category,
                "room_type": room_type,
                "week_id": week_id or "current"
            },
            "summary": {
                "total_properties": len(filtered_properties),
                "active_properties": len(active_properties),
                "deleted_properties": len(filtered_properties) - len(active_properties),
                "new_properties": 0,
                "avg_rent_all": round(avg_rent),
                "min_rent": min_rent,
                "max_rent": max_rent,
                "avg_area": round(avg_area, 1)
            },
            "properties": filtered_properties,
            "room_type_analysis": room_type_analysis,
            "data_source": "google_drive" if drive_available else "local"
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

class ResetRequest(BaseModel):
    password: str

@app.post("/api/admin/reset-database")
async def reset_database(request: ResetRequest):
    """é‡ç½®æ•¸æ“šåº«ä¸¦é‡æ–°æƒæ CSV"""
    if request.password != "1234":
        raise HTTPException(status_code=403, detail="å¯†ç¢¼éŒ¯èª¤")
    
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("DELETE FROM csv_index")
        cursor.execute("DELETE FROM versions")
        conn.commit()
        conn.close()
        scan_available_csv_files()
        return {"status": "success", "message": "æ•¸æ“šåº«å·²é‡ç½®ä¸¦é‡æ–°æƒæ CSV æ–‡ä»¶"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/admin/database-status")
async def database_status():
    """ç²å–æ•¸æ“šåº«ç‹€æ…‹"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM csv_index")
        csv_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT SUM(record_count) FROM csv_index")
        total_records = cursor.fetchone()[0] or 0
        
        cursor.execute("SELECT week_id, upload_date FROM versions ORDER BY week_id DESC")
        versions = [{"week_id": row[0], "upload_date": row[1]} for row in cursor.fetchall()]
        
        cursor.execute("SELECT filename, district, building_type, property_category, week_id, record_count FROM csv_index ORDER BY district, building_type, property_category")
        csv_files = [{"filename": row[0], "district": row[1], "building_type": row[2], "property_category": row[3], "week_id": row[4], "record_count": row[5]} for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            "status": "success",
            "database": {
                "csv_files_count": csv_count,
                "total_records": total_records,
                "versions_count": len(versions),
                "versions": versions,
                "csv_files": csv_files
            },
            "google_drive": {
                "available": drive_available,
                "folder_id": drive_folder_id if drive_available else None
            },
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/admin/rescan-csv")
async def rescan_csv():
    """é‡æ–°æƒæ CSV æ–‡ä»¶"""
    try:
        scan_available_csv_files()
        
        # è¿”å›æƒæçµæœçš„è©³ç´°è³‡è¨Š
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM csv_index")
        count = cursor.fetchone()[0]
        cursor.execute("SELECT DISTINCT city FROM csv_index")
        cities = [row[0] for row in cursor.fetchall()]
        conn.close()
        
        return {
            "status": "success", 
            "message": "CSV æ–‡ä»¶å·²é‡æ–°æƒæ",
            "indexed_files": count,
            "cities": cities,
            "drive_available": drive_available,
            "drive_folder_id": drive_folder_id
        }
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "message": str(e),
            "traceback": traceback.format_exc()
        }

@app.get("/api/admin/drive-status")
async def get_drive_status():
    """è¨ºæ–· Google Drive é€£æ¥ç‹€æ…‹"""
    result = {
        "drive_available": drive_available,
        "drive_folder_id": drive_folder_id,
        "drive_folder_name": GOOGLE_DRIVE_FOLDER_NAME,
        "has_service": drive_service is not None,
        "env_key_exists": os.getenv('GOOGLE_DRIVE_KEY_JSON') is not None,
        "files_found": [],
        "error": None
    }
    
    if drive_available and drive_folder_id:
        try:
            files = list_google_drive_files(drive_folder_id)
            result["files_found"] = files[:50]  # åªè¿”å›å‰ 50 å€‹æª”æ¡ˆ
            result["total_files"] = len(files)
        except Exception as e:
            result["error"] = str(e)
    
    return result

# éœæ…‹æ–‡ä»¶æœå‹™
static_dir = os.path.dirname(__file__)
if os.path.exists(static_dir):
    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")
